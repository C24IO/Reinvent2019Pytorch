{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference Source: PyTorch Example from SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_lstm_word_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See [the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet the execution role for the notebook instance. This is the IAM role that you created when you created your notebook instance. You pass the role to the tuning job.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "'''\n",
    "A session stores configuration state and allows you to create service clients and resources.\n",
    "sagemaker.session.Session - AWS service calls are delegated to an underlying Boto3 session, \n",
    "which by default is initialized using the AWS configuration chain. \n",
    "When you make an Amazon SageMaker API call that accesses an S3 bucket location and one is not specified, \n",
    "the Session creates a default bucket based on a naming convention which includes the current AWS account ID.\n",
    "'''\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "'''\n",
    "Form of the name of the bucket - sagemaker-{region}-{AWS account ID} Return the name of the default bucket to use in relevant Amazon SageMaker interactions.\n",
    "\n",
    "'''\n",
    "\n",
    "prefix = 'sagemaker/DEMO-pytorch-rnn-lstm'\n",
    "\n",
    "'''\n",
    "Used later\n",
    "'''\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "'''\n",
    "Get the execution role for the notebook instance. This is the IAM role that you created when you created your notebook instance. You pass the role to the tuning job.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "As mentioned above we are going to use [the wikitext-2 raw data](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). This data is from Wikipedia and is licensed CC-BY-SA-3.0. Before you use this data for any other purpose than this example, you should understand the data license, described at https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "This dataset is provided by SalesForce, The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. What we have here is a good example of how English language flows.\n",
    "\n",
    "### Examples\n",
    "= Gold dollar =\n",
    "\n",
    " The gold dollar or gold one @-@ dollar piece was a coin struck as a regular issue by the United States Bureau of the Mint from 1849 to 1889 . The coin had three types over its lifetime , all designed by Mint Chief Engraver James B. Longacre . The Type 1 issue had the smallest diameter of any United States coin ever minted .\n",
    " A gold dollar had been proposed several times in the 1830s and 1840s , but was not initially adopted . Congress was finally galvanized into action by the increased supply of bullion caused by the California gold rush , and in 1849 authorized a gold dollar . In its early years , silver coins were being hoarded or exported , and the gold dollar found a ready place in commerce . Silver again circulated after Congress in 1853 required that new coins of that metal be made lighter , and the gold dollar became a rarity in commerce even before federal coins vanished from circulation because of the economic disruption caused by the American Civil War .\n",
    " \n",
    " = Super Mario Land =\n",
    "\n",
    " Super Mario Land is a 1989 side @-@ scrolling platform video game , the first in the Super Mario Land series , developed and published by Nintendo as a launch title for their Game Boy handheld game console . In gameplay similar to that of the 1985 Super Mario Bros. , but resized for the smaller device 's screen , the player advances Mario to the end of 12 levels by moving to the right and jumping across platforms to avoid enemies and pitfalls . Unlike other Mario games , Super Mario Land is set in Sarasaland , a new environment depicted in line art , and Mario pursues Princess Daisy . The game introduces two Gradius @-@ style shooter levels .\n",
    " At Nintendo CEO Hiroshi Yamauchi 's request , Game Boy creator Gunpei Yokoi 's Nintendo R & D1 developed a Mario game to sell the new console . It was the first portable version of Mario and the first to be made without Mario creator and Yokoi protégé Shigeru Miyamoto . Accordingly , the development team shrunk Mario gameplay elements for the device and used some elements inconsistently from the series . Super Mario Land was expected to showcase the console until Nintendo of America bundled Tetris with new Game Boys . The game launched alongside the Game Boy first in Japan ( April 1989 ) and later worldwide . Super Mario Land was later rereleased for the Nintendo 3DS via Virtual Console in 2011 again as a launch title , which featured some tweaks to the game 's presentation .\n",
    " Initial reviews were laudatory . Reviewers were satisfied with the smaller Super Mario Bros. , but noted its short length . They considered it among the best of the Game Boy launch titles . The handheld console became an immediate success and Super Mario Land ultimately sold over 18 million copies , more than that of Super Mario Bros. 3 . Both contemporaneous and retrospective reviewers praised the game 's soundtrack . Later reviews were critical of the compromises made in development and noted Super Mario Land 's deviance from series norms . The game begot a series of sequels , including the 1992 Super Mario Land 2 : 6 Golden Coins , 1994 Wario Land : Super Mario Land 3 , and 2011 Super Mario 3D Land , though many of the original 's mechanics were not revisited . The game was included in several top Game Boy game lists and debuted Princess Daisy as a recurring Mario series character .\n",
    " \n",
    "= = = Sinclair Scientific Programmable = = =\n",
    "\n",
    " The Sinclair Scientific Programmable was introduced in 1975 , with the same case as the Sinclair Oxford . It was larger than the Scientific , at 73 by 155 by 34 millimetres ( 2 @.@ 9 in × 6 @.@ 1 in × 1 @.@ 3 in ) , and used a larger  battery , but could also be powered by mains electricity .\n",
    " It had 24 @-@ step programming abilities , which meant it was highly limited for many purposes . It also lacked functions for the natural logarithm and exponential function . Constants used in programs were required to be integers , and the programming was wasteful , with start and end quotes needed to use a constant in a program .\n",
    " However , included with the calculator was a library of over 120 programs that that performed common operations in mathematics , geometry , statistics , finance , physics , electronics , engineering , as well as fluid mechanics and materials science . The full library of standard programs contained over 400 programs in the Sinclair Program Library .\n",
    "\n",
    "### Dataset statistics\n",
    "In comparison to the Mikolov processed version of the Penn Treebank (PTB), the WikiText datasets are larger. WikiText-2 aims to be of a similar size to the PTB while WikiText-103 contains all articles extracted from Wikipedia. The WikiText datasets also retain numbers (as opposed to replacing them with N), case (as opposed to all text being lowercased), and punctuation (as opposed to stripping them out).\n",
    "\n",
    "![Dataset statistics](../img/dataset-statistics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-raw-v1.zip\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-11-26 22:15:28--  http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)... 52.216.144.131\n",
      "Connecting to research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)|52.216.144.131|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip.1’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  327K 14s\n",
      "    50K .......... .......... .......... .......... ..........  2% 2.00M 8s\n",
      "   100K .......... .......... .......... .......... ..........  3%  950K 7s\n",
      "   150K .......... .......... .......... .......... ..........  4%  125M 5s\n",
      "   200K .......... .......... .......... .......... ..........  5%  661K 5s\n",
      "   250K .......... .......... .......... .......... ..........  6% 75.1M 4s\n",
      "   300K .......... .......... .......... .......... ..........  7% 58.9M 4s\n",
      "   350K .......... .......... .......... .......... ..........  8%  109M 3s\n",
      "   400K .......... .......... .......... .......... ..........  9% 2.12M 3s\n",
      "   450K .......... .......... .......... .......... .......... 10%  970K 3s\n",
      "   500K .......... .......... .......... .......... .......... 11%  114M 3s\n",
      "   550K .......... .......... .......... .......... .......... 13%  118M 3s\n",
      "   600K .......... .......... .......... .......... .......... 14% 20.4M 2s\n",
      "   650K .......... .......... .......... .......... .......... 15%  144M 2s\n",
      "   700K .......... .......... .......... .......... .......... 16%  111M 2s\n",
      "   750K .......... .......... .......... .......... .......... 17%  140M 2s\n",
      "   800K .......... .......... .......... .......... .......... 18%  127M 2s\n",
      "   850K .......... .......... .......... .......... .......... 19% 2.39M 2s\n",
      "   900K .......... .......... .......... .......... .......... 20%  974K 2s\n",
      "   950K .......... .......... .......... .......... .......... 21% 92.9M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 22%  104M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 23%  104M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 24%  377M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26%  370M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27%  363M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28%  311M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29% 31.7M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30% 95.3M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31% 82.2M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32%  140M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33%  378M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34%  382M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 35%  323M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 36%  356M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 37%  371M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39% 2.50M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40%  328M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41%  969K 1s\n",
      "  1900K .......... .......... .......... .......... .......... 42%  169M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 43%  106M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 44%  105M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 45%  124M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 46%  133M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 47%  124M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 48%  244M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 49%  193M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 50%  106M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 52% 77.5M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 53%  112M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 54%  102M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55%  275M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56%  329M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57%  372M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58%  366M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 59%  361M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 60%  321M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 61%  360M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 62%  363M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 63%  377M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65%  324M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66%  371M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67%  353M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68%  377M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69%  313M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 70%  360M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 71%  374M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 72% 2.77M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 73%  370M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 74%  973K 0s\n",
      "  3450K .......... .......... .......... .......... .......... 75% 92.0M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 76%  117M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78%  124M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79%  117M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80% 90.6M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81%  109M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 82%  105M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 83% 83.7M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 84% 88.2M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 85%  120M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 86%  372M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 87%  390M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 88%  303M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90%  374M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91%  383M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92%  373M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93%  313M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 94%  387M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 95%  370M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 96%  368M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 97%  301M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 98%  374M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 99%  364M 0s\n",
      "  4600K ..........                                            100%  379M=0.6s\n",
      "\n",
      "2019-11-26 22:15:29 (7.28 MB/s) - ‘wikitext-2-raw-v1.zip.1’ saved [4721645/4721645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
    "unzip -n wikitext-2-raw-v1.zip\n",
    "cd wikitext-2-raw\n",
    "mv wiki.test.raw test && mv wiki.train.raw train && mv wiki.valid.raw valid\n",
    "# Moving the pre-divided datasets into Test, Train and Validation directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview what data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n"
     ]
    }
   ],
   "source": [
    "!head -5 wikitext-2-raw/train\n",
    "#Lets see how the train dataset looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-111652037296/sagemaker/DEMO-pytorch-rnn-lstm\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='wikitext-2-raw', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "'''\n",
    "S3 object key name prefix (default: ‘data’). S3 uses the prefix to create a directory structure for the bucket content that it display in the S3 console.\n",
    "\n",
    "Tree of the datasets - \n",
    "\n",
    "├── wikitext-2-raw\n",
    "│   ├── test\n",
    "│   ├── train\n",
    "│   └── valid\n",
    "'''\n",
    "\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_OUTPUT_DATA_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may\n",
    "  include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed\n",
    "  and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method,\n",
    "the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "The script that we will use in this example is stored in GitHub repo \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts](https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts), \n",
    "under the branch `training-scripts`. It is a public repo so we don't need authentication to access it. Let's specify the `git_config` argument here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. \n",
    "\n",
    "For example, the script run by this notebook: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py). \n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current example we also need to provide source directory, because training script imports data and model classes from other modules. The source directory is \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/). We should provide 'pytorch-rnn-scripts' for `source_dir` when creating the Estimator object, which is a relative path inside the Git repository. \n",
    "\n",
    "\n",
    "Lets see the training script in details - this training script is located here - \n",
    "\n",
    "```bash\n",
    "├── pytorch-rnn-scripts\n",
    "│   ├── data.py\n",
    "│   ├── generate.py\n",
    "│   ├── __init__.py\n",
    "│   ├── rnn.py\n",
    "│   └── train.py\n",
    "```\n",
    "\n",
    "```python\n",
    "import data\n",
    "```\n",
    "\n",
    "Here we import data.py, data.py has functions for tokenizing and creating a corpus for consumptions. A few relevant details here - [tokens](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15#issuecomment-246106807)\n",
    "\n",
    "Then we have hyperparamters being passed to this script, you can see this in the training blob\n",
    "\n",
    "\n",
    "```python\n",
    "# Hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', type=bool, default=False,\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "```\n",
    "Then we have details of file paths - \n",
    "\n",
    "```python\n",
    "# Data and model checkpoints/otput directories from the container environment\n",
    "parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "```\n",
    "\n",
    "Here are some logs from when a job like this was run - \n",
    "\n",
    "```json\n",
    "SM_TRAINING_ENV=\n",
    "{\n",
    "    \"additional_framework_parameters\": {},\n",
    "    \"channel_input_dirs\": {\n",
    "        \"training\": \"/opt/ml/input/data/training\"\n",
    "    },\n",
    "    \"current_host\": \"algo-1\",\n",
    "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
    "    \"hosts\": [\n",
    "        \"algo-1\"\n",
    "    ],\n",
    "    \"hyperparameters\": {\n",
    "        \"epochs\": 6,\n",
    "        \"tied\": true\n",
    "    },\n",
    "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
    "    \"input_data_config\": {\n",
    "        \"training\": {\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"S3DistributionType\": \"FullyReplicated\",\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        }\n",
    "    },\n",
    "    \"input_dir\": \"/opt/ml/input\",\n",
    "    \"is_master\": true,\n",
    "    \"job_name\": \"sagemaker-pytorch-2019-11-26-19-32-08-962\",\n",
    "    \"log_level\": 20,\n",
    "    \"master_hostname\": \"algo-1\",\n",
    "    \"model_dir\": \"/opt/ml/model\",\n",
    "    \"module_dir\": \"s3://sagemaker-us-west-2-111652037296/sagemaker-pytorch-2019-11-26-19-32-08-962/source/sourcedir.tar.gz\",\n",
    "    \"module_name\": \"train\",\n",
    "    \"network_interface_name\": \"eth0\",\n",
    "    \"num_cpus\": 4,\n",
    "    \"num_gpus\": 1,\n",
    "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
    "    \"output_dir\": \"/opt/ml/output\",\n",
    "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
    "    \"resource_config\": {\n",
    "        \"current_host\": \"algo-1\",\n",
    "        \"hosts\": [\n",
    "            \"algo-1\"\n",
    "        ],\n",
    "        \"network_interface_name\": \"eth0\"\n",
    "    },\n",
    "    \"user_entry_point\": \"train.py\"\n",
    "}\n",
    "```\n",
    "\n",
    "Here are some environment variables - \n",
    "\n",
    "```json\n",
    "SM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\n",
    "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
    "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
    "SM_HP_TIED=true\n",
    "SM_HP_EPOCHS=6\n",
    "PYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
    "Invoking script with the following command:\n",
    "/usr/bin/python -m train --epochs 6 --tied True\n",
    "\n",
    "Namespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\n",
    "```\n",
    "\n",
    "You can find the logs by going to the training jobs in the Amazon SageMaker Dashboard\n",
    "\n",
    "```python\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "```\n",
    "\n",
    "This seed you can seed was 1111 in the above example. You can use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA). Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds. However, in order to make computations deterministic on your specific problem on one specific platform and PyTorch release, there are a couple of steps to take. This is one of these steps. More details [here(https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "\n",
    "\n",
    "Now we load the corpus created by data.py\n",
    "\n",
    "```python\n",
    "print('Load data')\n",
    "corpus = data.Corpus(args.data_dir)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model in Pytorch\n",
    "\n",
    "Now at this stage after a bit more setup - we load the model \n",
    "\n",
    "```python\n",
    "ntokens = len(corpus.dictionary)\n",
    "rnn_type = 'LSTM'\n",
    "model = RNNModel(rnn_type, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
    "```\n",
    "\n",
    "We have defined the model in - rnn.py\n",
    "\n",
    "We are using LSTM which is a variant of RNN\n",
    "\n",
    "![The LSTM Cell](../img/The_LSTM_cell-600.400.png)\n",
    "\n",
    "Soure of image - [Guillaume Chevalier from Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#/media/File:The_LSTM_cell.png)\n",
    "\n",
    "Now you can see how the above architecture is setup, we can understand this better using the following image as a resource. \n",
    "\n",
    "![The LSTM Cell - in series](../img/nct-seq2seq.png)\n",
    "\n",
    "Soure of image - [Deep Learning for Chatbots, Part 1 – Introduction](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ```ml.p2.xlarge``` instance. As you can see in this example you can also specify hyperparameters. \n",
    "\n",
    "Here we are using a prebuilt container for training our script, if you want to create your own please navigate to - https://github.com/aws/sagemaker-pytorch-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    source_dir='pytorch-rnn-scripts',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'tied': True\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Hosting script\n",
    "We are going to provide custom implementation of `model_fn`, `input_fn`, `output_fn` and `predict_fn` hosting functions in a separate file, which is in the same Git repo as the training script: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py). \n",
    "We will use Git integration for hosting too since the hosting code is also in the Git repo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also put your training and hosting code in the same file but you would need to add a main guard (`if __name__=='__main__':`) for the training code, so that the container does not inadvertently run it at the wrong point in execution during hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "The PyTorch model uses a npy serializer and deserializer by default. For this example, since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our `generate` script requires model and data classes from source directory), an IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3dcd06f7b22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyTorchModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrained_model_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ModelArtifacts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S3ModelArtifacts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.0.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='pytorch-rnn-scripts',\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the `sagemaker.pytorch.model.PyTorchModel.deploy` method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    'seed': 200,\n",
    "    'temperature': 510.0,\n",
    "    'words': 200\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
