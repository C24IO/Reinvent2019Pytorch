{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference Source: PyTorch Example from SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_lstm_word_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See [the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-rnn-lstm'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "As mentioned above we are going to use [the wikitext-2 raw data](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). This data is from Wikipedia and is licensed CC-BY-SA-3.0. Before you use this data for any other purpose than this example, you should understand the data license, described at https://creativecommons.org/licenses/by-sa/3.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-11-18 05:22:56--  http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)... 52.216.243.124\n",
      "Connecting to research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)|52.216.243.124|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  320K 14s\n",
      "    50K .......... .......... .......... .......... ..........  2% 2.21M 8s\n",
      "   100K .......... .......... .......... .......... ..........  3%  878K 7s\n",
      "   150K .......... .......... .......... .......... ..........  4%  314M 5s\n",
      "   200K .......... .......... .......... .......... ..........  5%  643K 5s\n",
      "   250K .......... .......... .......... .......... ..........  6% 88.1M 5s\n",
      "   300K .......... .......... .......... .......... ..........  7% 81.2M 4s\n",
      "   350K .......... .......... .......... .......... ..........  8%  197M 3s\n",
      "   400K .......... .......... .......... .......... ..........  9% 2.32M 3s\n",
      "   450K .......... .......... .......... .......... .......... 10%  895K 3s\n",
      "   500K .......... .......... .......... .......... .......... 11%  104M 3s\n",
      "   550K .......... .......... .......... .......... .......... 13%  130M 3s\n",
      "   600K .......... .......... .......... .......... .......... 14% 97.2M 2s\n",
      "   650K .......... .......... .......... .......... .......... 15%  127M 2s\n",
      "   700K .......... .......... .......... .......... .......... 16%  170M 2s\n",
      "   750K .......... .......... .......... .......... .......... 17%  148M 2s\n",
      "   800K .......... .......... .......... .......... .......... 18%  179M 2s\n",
      "   850K .......... .......... .......... .......... .......... 19% 2.54M 2s\n",
      "   900K .......... .......... .......... .......... .......... 20%  893K 2s\n",
      "   950K .......... .......... .......... .......... .......... 21%  108M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 22% 92.7M 2s\n",
      "  1050K .......... .......... .......... .......... .......... 23%  130M 2s\n",
      "  1100K .......... .......... .......... .......... .......... 24%  121M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26%  144M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27%  101M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28%  140M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29%  111M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30%  270M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31%  137M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32%  150M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33%  142M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34%  388M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 35%  338M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 36%  390M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 37%  345M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39% 2.58M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40%  323M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41%  926K 1s\n",
      "  1900K .......... .......... .......... .......... .......... 42% 96.6M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 43%  105M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 44%  104M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 45%  151M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 46%  177M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 47%  117M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 48%  109M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 49%  116M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 50%  116M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 52%  125M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 53% 98.3M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 54%  121M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55%  120M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56%  217M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57%  331M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58%  381M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 59%  373M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 60%  372M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 61%  284M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 62%  367M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 63%  368M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65%  355M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66%  327M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67%  367M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68%  364M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69%  379M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 70%  326M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 71%  373M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 72% 3.24M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 73%  905K 0s\n",
      "  3400K .......... .......... .......... .......... .......... 74% 98.6M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 75%  147M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 76%  170M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78%  147M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79%  128M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80% 90.5M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81%  132M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 82%  208M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 83%  119M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 84%  142M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 85%  139M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 86%  102M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 87%  199M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 88%  140M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90%  148M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91%  163M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92%  120M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93%  103M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 94%  115M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 95%  383M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 96%  136M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 97%  254M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 98%  317M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 99%  378M 0s\n",
      "  4600K ..........                                            100%  394M=0.6s\n",
      "\n",
      "2019-11-18 05:22:57 (7.11 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
    "unzip -n wikitext-2-raw-v1.zip\n",
    "cd wikitext-2-raw\n",
    "mv wiki.test.raw test && mv wiki.train.raw train && mv wiki.valid.raw valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview what data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n"
     ]
    }
   ],
   "source": [
    "!head -5 wikitext-2-raw/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-111652037296/sagemaker/DEMO-pytorch-rnn-lstm\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='wikitext-2-raw', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_OUTPUT_DATA_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may\n",
    "  include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed\n",
    "  and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method,\n",
    "the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. \n",
    "\n",
    "In this notebook example, we will use Git integration. That is, you can specify a training script that is stored in a GitHub, CodeCommit or other Git repository as the entry point for the estimator, so that you don't have to download the scripts locally. If you do so, source directory and dependencies should be in the same repo if they are needed.\n",
    "\n",
    "To use Git integration, pass a dict `git_config` as a parameter when you create the `PyTorch` Estimator object. In the `git_config` parameter, you specify the fields `repo`, `branch` and `commit` to locate the specific repo you want to use. If authentication is required to access the repo, you can specify fields `2FA_enabled`, `username`, `password` and token accordingly.\n",
    "\n",
    "The script that we will use in this example is stored in GitHub repo \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts](https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts), \n",
    "under the branch `training-scripts`. It is a public repo so we don't need authentication to access it. Let's specify the `git_config` argument here: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git_config = {'repo': 'https://github.com/awslabs/amazon-sagemaker-examples.git', 'branch': 'training-scripts'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not specify `commit` in `git_config` here, in which case the latest commit of the specified repo and branch will be used by default. \n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. \n",
    "\n",
    "For example, the script run by this notebook: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py). \n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current example we also need to provide source directory, because training script imports data and model classes from other modules. The source directory is \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/). We should provide 'pytorch-rnn-scripts' for `source_dir` when creating the Estimator object, which is a relative path inside the Git repository. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ```ml.p2.xlarge``` instance. As you can see in this example you can also specify hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    source_dir='pytorch-rnn-scripts',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'tied': True\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-18 05:22:59 Starting - Starting the training job...\n",
      "2019-11-18 05:23:01 Starting - Launching requested ML instances......\n",
      "2019-11-18 05:24:29 Starting - Preparing the instances for training.........\n",
      "2019-11-18 05:25:33 Downloading - Downloading input data...\n",
      "2019-11-18 05:26:21 Training - Downloading the training image..............\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:39,600 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:39,624 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:40,237 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:40,535 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:40,535 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:40,536 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:40,536 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=20542 sha256=39304343654587df22df842755ef1ee4e1a6911ba608ca560df5d8efd40421e9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l__u1ey1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mWARNING: You are using pip version 19.3; however, version 19.3.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-11-18 05:28:42,680 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2019-11-18-05-22-58-697\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-111652037296/pytorch-training-2019-11-18-05-22-58-697/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":6,\"tied\":true}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-111652037296/pytorch-training-2019-11-18-05-22-58-697/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2019-11-18-05-22-58-697\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-111652037296/pytorch-training-2019-11-18-05-22-58-697/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/opt/conda/bin/python -m train --epochs 6 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\u001b[0m\n",
      "\u001b[31mLoad data\u001b[0m\n",
      "\n",
      "2019-11-18 05:28:38 Training - Training image download completed. Training in progress.\u001b[31mBatchify dataset\u001b[0m\n",
      "\u001b[31mBuild the model\u001b[0m\n",
      "\u001b[31mStarting training.\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 101.19 | loss  8.33 | ppl  4136.34\u001b[0m\n",
      "\u001b[31m| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 101.08 | loss  7.38 | ppl  1606.61\u001b[0m\n",
      "\u001b[31m| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 105.36 | loss  6.87 | ppl   964.80\u001b[0m\n",
      "\u001b[31m| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 98.25 | loss  6.68 | ppl   794.86\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 102.95 | loss  6.45 | ppl   632.29\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 99.76 | loss  6.39 | ppl   595.52\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 102.73 | loss  6.31 | ppl   548.38\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 103.90 | loss  6.29 | ppl   539.20\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 102.99 | loss  6.08 | ppl   436.73\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 102.59 | loss  6.05 | ppl   422.07\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 102.27 | loss  5.92 | ppl   371.01\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 98.81 | loss  5.96 | ppl   388.87\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 103.82 | loss  5.94 | ppl   379.04\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 102.91 | loss  5.83 | ppl   340.01\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 314.84s | valid loss  6.03 | valid ppl   416.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 1, 'lr': 20, 'val_loss': 6.030891246884169, 'val_ppl': 416.0856992612763}\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 101.99 | loss  5.83 | ppl   341.53\u001b[0m\n",
      "\u001b[31m| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 98.56 | loss  5.80 | ppl   330.97\u001b[0m\n",
      "\u001b[31m| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 102.12 | loss  5.60 | ppl   271.56\u001b[0m\n",
      "\u001b[31m| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 98.45 | loss  5.62 | ppl   277.13\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 99.56 | loss  5.52 | ppl   249.90\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 99.17 | loss  5.55 | ppl   257.81\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 104.56 | loss  5.59 | ppl   268.73\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 100.93 | loss  5.65 | ppl   284.16\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 100.28 | loss  5.48 | ppl   239.59\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 101.83 | loss  5.49 | ppl   241.56\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 101.40 | loss  5.39 | ppl   219.18\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 98.63 | loss  5.45 | ppl   233.92\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 98.58 | loss  5.46 | ppl   236.21\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 102.93 | loss  5.38 | ppl   217.57\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 310.25s | valid loss  5.75 | valid ppl   314.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 2, 'lr': 20, 'val_loss': 5.751095302971249, 'val_ppl': 314.53498278374815}\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 101.13 | loss  5.45 | ppl   232.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 100.24 | loss  5.45 | ppl   233.06\u001b[0m\n",
      "\u001b[31m| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 100.41 | loss  5.25 | ppl   191.17\u001b[0m\n",
      "\u001b[31m| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 100.94 | loss  5.30 | ppl   200.75\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 97.89 | loss  5.22 | ppl   185.25\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 98.74 | loss  5.27 | ppl   193.69\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 102.30 | loss  5.33 | ppl   207.32\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 103.55 | loss  5.40 | ppl   221.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 103.36 | loss  5.23 | ppl   187.49\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 101.96 | loss  5.26 | ppl   192.34\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 103.89 | loss  5.16 | ppl   173.74\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 101.56 | loss  5.23 | ppl   186.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 100.46 | loss  5.25 | ppl   190.77\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 104.51 | loss  5.18 | ppl   177.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 314.09s | valid loss  5.65 | valid ppl   283.93\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 3, 'lr': 20, 'val_loss': 5.648728454820682, 'val_ppl': 283.9302061087303}\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 102.88 | loss  5.25 | ppl   190.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 99.69 | loss  5.27 | ppl   193.59\u001b[0m\n",
      "\u001b[31m| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 101.45 | loss  5.06 | ppl   157.97\u001b[0m\n",
      "\u001b[31m| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 96.16 | loss  5.13 | ppl   168.75\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 99.60 | loss  5.05 | ppl   156.66\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 102.46 | loss  5.11 | ppl   165.33\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 100.80 | loss  5.18 | ppl   176.89\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 101.67 | loss  5.24 | ppl   189.53\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 102.26 | loss  5.09 | ppl   162.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 98.29 | loss  5.12 | ppl   167.23\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 99.95 | loss  5.02 | ppl   151.02\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 103.02 | loss  5.09 | ppl   162.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 103.36 | loss  5.12 | ppl   166.55\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 97.89 | loss  5.05 | ppl   155.38\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 311.09s | valid loss  5.60 | valid ppl   270.80\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 4, 'lr': 20, 'val_loss': 5.601393558680628, 'val_ppl': 270.803525200536}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 100.16 | loss  5.12 | ppl   167.99\u001b[0m\n",
      "\u001b[31m| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 100.01 | loss  5.14 | ppl   170.05\u001b[0m\n",
      "\u001b[31m| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 101.26 | loss  4.94 | ppl   139.46\u001b[0m\n",
      "\u001b[31m| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 99.55 | loss  5.00 | ppl   149.02\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 105.54 | loss  4.95 | ppl   140.72\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 105.39 | loss  5.00 | ppl   148.11\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 102.46 | loss  5.07 | ppl   158.69\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 101.12 | loss  5.14 | ppl   170.79\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 103.70 | loss  4.98 | ppl   145.98\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 104.99 | loss  5.02 | ppl   151.35\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 102.08 | loss  4.91 | ppl   135.80\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 101.99 | loss  4.98 | ppl   145.65\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 101.45 | loss  5.01 | ppl   149.46\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 101.64 | loss  4.95 | ppl   140.62\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 315.24s | valid loss  5.58 | valid ppl   265.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 5, 'lr': 20, 'val_loss': 5.582009914122485, 'val_ppl': 265.60491272120066}\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 101.39 | loss  5.02 | ppl   151.79\u001b[0m\n",
      "\u001b[31m| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 102.82 | loss  5.04 | ppl   153.73\u001b[0m\n",
      "\u001b[31m| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 106.70 | loss  4.84 | ppl   126.57\u001b[0m\n",
      "\u001b[31m| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 103.51 | loss  4.91 | ppl   135.96\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 106.26 | loss  4.86 | ppl   129.46\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 105.37 | loss  4.92 | ppl   136.79\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 107.38 | loss  4.98 | ppl   145.84\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 100.62 | loss  5.05 | ppl   156.77\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 102.93 | loss  4.91 | ppl   135.50\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 102.72 | loss  4.94 | ppl   139.67\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 104.95 | loss  4.84 | ppl   125.85\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 103.57 | loss  4.90 | ppl   134.81\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 101.15 | loss  4.93 | ppl   138.93\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 101.76 | loss  4.88 | ppl   131.10\u001b[0m\n",
      "\n",
      "2019-11-18 06:00:49 Uploading - Uploading generated training model\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 319.49s | valid loss  5.56 | valid ppl   258.59\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 6, 'lr': 20, 'val_loss': 5.555247321546878, 'val_ppl': 258.5909117167824}\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  5.57 | test ppl   262.91\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m2019-11-18 06:00:48,376 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-11-18 06:01:10 Completed - Training job completed\n",
      "Training seconds: 2137\n",
      "Billable seconds: 2137\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Hosting script\n",
    "We are going to provide custom implementation of `model_fn`, `input_fn`, `output_fn` and `predict_fn` hosting functions in a separate file, which is in the same Git repo as the training script: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py). \n",
    "We will use Git integration for hosting too since the hosting code is also in the Git repo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also put your training and hosting code in the same file but you would need to add a main guard (`if __name__=='__main__':`) for the training code, so that the container does not inadvertently run it at the wrong point in execution during hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "The PyTorch model uses a npy serializer and deserializer by default. For this example, since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our `generate` script requires model and data classes from source directory), an IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.0.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='pytorch-rnn-scripts',\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the `sagemaker.pytorch.model.PyTorchModel.deploy` method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422nd wetland Potro integrating Eighty Demand lanyard sanctified Cévennes agendas detonations apology\n",
      "Escorial DeMille Charlynch helix Zeffirelli Victorien Hessian PRG imitators sheath barbershops Suresnes\n",
      "classifications roaming Alford RPGs guts Marshall Godrich Briefing Branislav 560 Roscoe souls\n",
      "beheaded Borough AIL Shoko Cowper Potworowski goalkeepers Kenith Roumette Ortona Ismailia S.259\n",
      "Wreath partied Bag survived applications annealed fisherman Plains luncheon Đình Tokyo heckles\n",
      "egos Dobson Aag HevyDevy Investigates able Hoodoo Hendrie quatre Needled Prussian visitor\n",
      "Hashomer Redfearn Matteino Stjepan Yiwu Bree roof Surdas simpler Measure bey 411\n",
      "Babang widened Altrock Kulukundis Maverick Wing startling Kostrzewska Featley emails yes Odetta\n",
      "Venture grammars Nachtigall Franka silence Kommunistblad 837 emcee 562 hOWLetts deducted reproductive\n",
      "traveller difficult Rogatti 90s Casino Tracey Screen sabotaging Udell abdicated anglica College\n",
      "lengthy Yuji Nonsense Forth realistic Ioke Quitman repairs Isonomy Costanza Eternia wingers\n",
      "moneyed interferometer impoverishing medicine sentimentality legume 239Pu turning obtains Beautiful Li Locks\n",
      "Sackheim stimmi bombardment flashy hibernate millinery Prover Celeste memoir declination Venu pectoral\n",
      "bright inamyloidy Silverman 549th liability Sangidu chimes Nye Pursey Katalog Gowri morphs\n",
      "condense Tecomán bipinnate Gilmore Marburg Sahel Bains allure spatializer Pogi dole Spence\n",
      "achieving satirist tripped clarity electrocyclizations Roussel Megabus frustrating lagena paratransgenesis neater Forgive\n",
      "1650 Yanow lethal rabbit jolly enactments inventory callback \n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    'seed': 200,\n",
    "    'temperature': 510.0,\n",
    "    'words': 200\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
