{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level language modeling using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference Source: PyTorch Example from SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/pytorch_lstm_word_language_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See [the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the sagemaker.get_execution_role() with appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "'''\n",
    "A session stores configuration state and allows you to create service clients and resources.\n",
    "sagemaker.session.Session - AWS service calls are delegated to an underlying Boto3 session, \n",
    "which by default is initialized using the AWS configuration chain. \n",
    "When you make an Amazon SageMaker API call that accesses an S3 bucket location and one is not specified, \n",
    "the Session creates a default bucket based on a naming convention which includes the current AWS account ID.\n",
    "'''\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "'''\n",
    "Form of the name of the bucket - sagemaker-{region}-{AWS account ID} Return the name of the default bucket to use in relevant Amazon SageMaker interactions.\n",
    "\n",
    "'''\n",
    "\n",
    "prefix = 'sagemaker/DEMO-pytorch-rnn-lstm'\n",
    "\n",
    "'''\n",
    "Used later\n",
    "'''\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "'''\n",
    "Get the execution role for the notebook instance. This is the IAM role that you created when you created your notebook instance. You pass the role to the tuning job.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "As mentioned above we are going to use [the wikitext-2 raw data](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). This data is from Wikipedia and is licensed CC-BY-SA-3.0. Before you use this data for any other purpose than this example, you should understand the data license, described at https://creativecommons.org/licenses/by-sa/3.0/\n",
    "\n",
    "This dataset is provided by SalesForce, The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. What we have here is a good example of how English language flows.\n",
    "\n",
    "### Examples\n",
    "= Gold dollar =\n",
    "\n",
    " The gold dollar or gold one @-@ dollar piece was a coin struck as a regular issue by the United States Bureau of the Mint from 1849 to 1889 . The coin had three types over its lifetime , all designed by Mint Chief Engraver James B. Longacre . The Type 1 issue had the smallest diameter of any United States coin ever minted .\n",
    " A gold dollar had been proposed several times in the 1830s and 1840s , but was not initially adopted . Congress was finally galvanized into action by the increased supply of bullion caused by the California gold rush , and in 1849 authorized a gold dollar . In its early years , silver coins were being hoarded or exported , and the gold dollar found a ready place in commerce . Silver again circulated after Congress in 1853 required that new coins of that metal be made lighter , and the gold dollar became a rarity in commerce even before federal coins vanished from circulation because of the economic disruption caused by the American Civil War .\n",
    " \n",
    " = Super Mario Land =\n",
    "\n",
    " Super Mario Land is a 1989 side @-@ scrolling platform video game , the first in the Super Mario Land series , developed and published by Nintendo as a launch title for their Game Boy handheld game console . In gameplay similar to that of the 1985 Super Mario Bros. , but resized for the smaller device 's screen , the player advances Mario to the end of 12 levels by moving to the right and jumping across platforms to avoid enemies and pitfalls . Unlike other Mario games , Super Mario Land is set in Sarasaland , a new environment depicted in line art , and Mario pursues Princess Daisy . The game introduces two Gradius @-@ style shooter levels .\n",
    " At Nintendo CEO Hiroshi Yamauchi 's request , Game Boy creator Gunpei Yokoi 's Nintendo R & D1 developed a Mario game to sell the new console . It was the first portable version of Mario and the first to be made without Mario creator and Yokoi protégé Shigeru Miyamoto . Accordingly , the development team shrunk Mario gameplay elements for the device and used some elements inconsistently from the series . Super Mario Land was expected to showcase the console until Nintendo of America bundled Tetris with new Game Boys . The game launched alongside the Game Boy first in Japan ( April 1989 ) and later worldwide . Super Mario Land was later rereleased for the Nintendo 3DS via Virtual Console in 2011 again as a launch title , which featured some tweaks to the game 's presentation .\n",
    " Initial reviews were laudatory . Reviewers were satisfied with the smaller Super Mario Bros. , but noted its short length . They considered it among the best of the Game Boy launch titles . The handheld console became an immediate success and Super Mario Land ultimately sold over 18 million copies , more than that of Super Mario Bros. 3 . Both contemporaneous and retrospective reviewers praised the game 's soundtrack . Later reviews were critical of the compromises made in development and noted Super Mario Land 's deviance from series norms . The game begot a series of sequels , including the 1992 Super Mario Land 2 : 6 Golden Coins , 1994 Wario Land : Super Mario Land 3 , and 2011 Super Mario 3D Land , though many of the original 's mechanics were not revisited . The game was included in several top Game Boy game lists and debuted Princess Daisy as a recurring Mario series character .\n",
    " \n",
    "= = = Sinclair Scientific Programmable = = =\n",
    "\n",
    " The Sinclair Scientific Programmable was introduced in 1975 , with the same case as the Sinclair Oxford . It was larger than the Scientific , at 73 by 155 by 34 millimetres ( 2 @.@ 9 in × 6 @.@ 1 in × 1 @.@ 3 in ) , and used a larger  battery , but could also be powered by mains electricity .\n",
    " It had 24 @-@ step programming abilities , which meant it was highly limited for many purposes . It also lacked functions for the natural logarithm and exponential function . Constants used in programs were required to be integers , and the programming was wasteful , with start and end quotes needed to use a constant in a program .\n",
    " However , included with the calculator was a library of over 120 programs that that performed common operations in mathematics , geometry , statistics , finance , physics , electronics , engineering , as well as fluid mechanics and materials science . The full library of standard programs contained over 400 programs in the Sinclair Program Library .\n",
    "\n",
    "### Dataset statistics\n",
    "In comparison to the Mikolov processed version of the Penn Treebank (PTB), the WikiText datasets are larger. WikiText-2 aims to be of a similar size to the PTB while WikiText-103 contains all articles extracted from Wikipedia. The WikiText datasets also retain numbers (as opposed to replacing them with N), case (as opposed to all text being lowercased), and punctuation (as opposed to stripping them out).\n",
    "\n",
    "![Dataset statistics](../img/dataset-statistics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-11-26 19:25:06--  http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)... 52.216.98.11\n",
      "Connecting to research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)|52.216.98.11|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  359K 13s\n",
      "    50K .......... .......... .......... .......... ..........  2% 2.21M 7s\n",
      "   100K .......... .......... .......... .......... ..........  3% 1.01M 6s\n",
      "   150K .......... .......... .......... .......... ..........  4% 71.1M 5s\n",
      "   200K .......... .......... .......... .......... ..........  5%  728K 5s\n",
      "   250K .......... .......... .......... .......... ..........  6% 82.5M 4s\n",
      "   300K .......... .......... .......... .......... ..........  7% 70.2M 3s\n",
      "   350K .......... .......... .......... .......... ..........  8% 81.5M 3s\n",
      "   400K .......... .......... .......... .......... ..........  9% 2.19M 3s\n",
      "   450K .......... .......... .......... .......... .......... 10% 1.07M 3s\n",
      "   500K .......... .......... .......... .......... .......... 11%  125M 3s\n",
      "   550K .......... .......... .......... .......... .......... 13%  118M 2s\n",
      "   600K .......... .......... .......... .......... .......... 14%  134M 2s\n",
      "   650K .......... .......... .......... .......... .......... 15%  181M 2s\n",
      "   700K .......... .......... .......... .......... .......... 16%  135M 2s\n",
      "   750K .......... .......... .......... .......... .......... 17%  116M 2s\n",
      "   800K .......... .......... .......... .......... .......... 18%  239M 2s\n",
      "   850K .......... .......... .......... .......... .......... 19% 2.28M 2s\n",
      "   900K .......... .......... .......... .......... .......... 20% 1.07M 2s\n",
      "   950K .......... .......... .......... .......... .......... 21%  104M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 22% 95.1M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 23%  160M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 24% 72.9M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26%  135M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27%  208M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28%  168M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29%  142M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30%  259M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31%  209M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32%  159M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33%  323M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34%  372M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 35%  359M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 36%  375M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 37%  305M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39% 2.44M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40%  370M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41% 1.08M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 42%  126M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 43%  103M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 44%  175M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 45%  176M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 46%  203M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 47%  208M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 48%  163M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 49%  210M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 50%  191M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 52%  135M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 53%  106M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 54%  204M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55%  165M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56%  221M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57%  164M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58%  383M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 59%  378M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 60%  362M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 61%  330M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 62%  335M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 63%  372M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65%  354M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66%  310M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67%  370M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68%  330M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69%  379M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 70%  325M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 71%  358M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 72% 2.79M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 73%  177M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 74% 1.08M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 75%  115M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 76%  227M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78%  240M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79%  301M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80%  190M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81%  109M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 82%  106M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 83%  116M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 84%  102M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 85% 89.2M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 86%  133M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 87%  123M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 88%  111M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90%  129M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91%  141M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92%  381M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93%  388M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 94%  292M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 95%  374M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 96%  373M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 97%  340M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 98%  310M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 99%  381M 0s\n",
      "  4600K ..........                                            100%  402M=0.6s\n",
      "\n",
      "2019-11-26 19:25:07 (7.97 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
    "unzip -n wikitext-2-raw-v1.zip\n",
    "cd wikitext-2-raw\n",
    "mv wiki.test.raw test && mv wiki.train.raw train && mv wiki.valid.raw valid\n",
    "# Moving the pre-divided datasets into Test, Train and Validation directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview what data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n"
     ]
    }
   ],
   "source": [
    "!head -5 wikitext-2-raw/train\n",
    "#Lets see how the train dataset looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-111652037296/sagemaker/DEMO-pytorch-rnn-lstm-2\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='wikitext-2-raw', bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "'''\n",
    "S3 object key name prefix (default: ‘data’). S3 uses the prefix to create a directory structure for the bucket content that it display in the S3 console.\n",
    "\n",
    "Tree of the datasets - \n",
    "\n",
    "├── wikitext-2-raw\n",
    "│   ├── test\n",
    "│   ├── train\n",
    "│   └── valid\n",
    "'''\n",
    "\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "We need to provide a training script that can run on the SageMaker platform. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_OUTPUT_DATA_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may\n",
    "  include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed\n",
    "  and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method,\n",
    "the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "The script that we will use in this example is stored in GitHub repo \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts](https://github.com/awslabs/amazon-sagemaker-examples/tree/training-scripts), \n",
    "under the branch `training-scripts`. It is a public repo so we don't need authentication to access it. Let's specify the `git_config` argument here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. \n",
    "\n",
    "For example, the script run by this notebook: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/train.py). \n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the current example we also need to provide source directory, because training script imports data and model classes from other modules. The source directory is \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/). We should provide 'pytorch-rnn-scripts' for `source_dir` when creating the Estimator object, which is a relative path inside the Git repository. \n",
    "\n",
    "\n",
    "Lets see the training script in details - this training script is located here - \n",
    "\n",
    "```bash\n",
    "├── pytorch-rnn-scripts\n",
    "│   ├── data.py\n",
    "│   ├── generate.py\n",
    "│   ├── __init__.py\n",
    "│   ├── rnn.py\n",
    "│   └── train.py\n",
    "```\n",
    "\n",
    "```python\n",
    "import data\n",
    "```\n",
    "\n",
    "Here we import data.py, data.py has functions for tokenizing and creating a corpus for consumptions. A few relevant details here - [tokens](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15#issuecomment-246106807)\n",
    "\n",
    "Then we have hyperparamters being passed to this script, you can see this in the training blob\n",
    "\n",
    "\n",
    "```python\n",
    "# Hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--lr', type=float, default=20,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--clip', type=float, default=0.25,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=40,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=35,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', type=bool, default=False,\n",
    "                    help='tie the word embedding and softmax weights')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n",
    "                    help='report interval')\n",
    "```\n",
    "Then we have details of file paths - \n",
    "\n",
    "```python\n",
    "# Data and model checkpoints/otput directories from the container environment\n",
    "parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "```\n",
    "\n",
    "Here are some logs from when a job like this was run - \n",
    "\n",
    "```json\n",
    "SM_TRAINING_ENV=\n",
    "{\n",
    "    \"additional_framework_parameters\": {},\n",
    "    \"channel_input_dirs\": {\n",
    "        \"training\": \"/opt/ml/input/data/training\"\n",
    "    },\n",
    "    \"current_host\": \"algo-1\",\n",
    "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
    "    \"hosts\": [\n",
    "        \"algo-1\"\n",
    "    ],\n",
    "    \"hyperparameters\": {\n",
    "        \"epochs\": 6,\n",
    "        \"tied\": true\n",
    "    },\n",
    "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
    "    \"input_data_config\": {\n",
    "        \"training\": {\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "            \"S3DistributionType\": \"FullyReplicated\",\n",
    "            \"TrainingInputMode\": \"File\"\n",
    "        }\n",
    "    },\n",
    "    \"input_dir\": \"/opt/ml/input\",\n",
    "    \"is_master\": true,\n",
    "    \"job_name\": \"sagemaker-pytorch-2019-11-26-19-32-08-962\",\n",
    "    \"log_level\": 20,\n",
    "    \"master_hostname\": \"algo-1\",\n",
    "    \"model_dir\": \"/opt/ml/model\",\n",
    "    \"module_dir\": \"s3://sagemaker-us-west-2-111652037296/sagemaker-pytorch-2019-11-26-19-32-08-962/source/sourcedir.tar.gz\",\n",
    "    \"module_name\": \"train\",\n",
    "    \"network_interface_name\": \"eth0\",\n",
    "    \"num_cpus\": 4,\n",
    "    \"num_gpus\": 1,\n",
    "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
    "    \"output_dir\": \"/opt/ml/output\",\n",
    "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
    "    \"resource_config\": {\n",
    "        \"current_host\": \"algo-1\",\n",
    "        \"hosts\": [\n",
    "            \"algo-1\"\n",
    "        ],\n",
    "        \"network_interface_name\": \"eth0\"\n",
    "    },\n",
    "    \"user_entry_point\": \"train.py\"\n",
    "}\n",
    "```\n",
    "\n",
    "Here are some environment variables - \n",
    "\n",
    "```json\n",
    "SM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\n",
    "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
    "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
    "SM_HP_TIED=true\n",
    "SM_HP_EPOCHS=6\n",
    "PYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
    "Invoking script with the following command:\n",
    "/usr/bin/python -m train --epochs 6 --tied True\n",
    "\n",
    "Namespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\n",
    "```\n",
    "\n",
    "You can find the logs by going to the training jobs in the Amazon SageMaker Dashboard\n",
    "\n",
    "```python\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "```\n",
    "\n",
    "This seed you can seed was 1111 in the above example. You can use torch.manual_seed() to seed the RNG for all devices (both CPU and CUDA). Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds. However, in order to make computations deterministic on your specific problem on one specific platform and PyTorch release, there are a couple of steps to take. This is one of these steps. More details [here(https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "The PyTorch class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script and source directory, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on ```ml.p2.xlarge``` instance. As you can see in this example you can also specify hyperparameters. \n",
    "\n",
    "Here we are using a prebuilt container for training our script, if you want to create your own please navigate to - https://github.com/aws/sagemaker-pytorch-container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    source_dir='pytorch-rnn-scripts',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'tied': True\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our PyTorch object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 19:32:10 Starting - Starting the training job...\n",
      "2019-11-26 19:32:11 Starting - Launching requested ML instances......\n",
      "2019-11-26 19:33:11 Starting - Preparing the instances for training.........\n",
      "2019-11-26 19:35:04 Downloading - Downloading input data\n",
      "2019-11-26 19:35:04 Training - Downloading the training image...\n",
      "2019-11-26 19:35:34 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:36,155 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:36,180 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:36,790 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:37,053 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:37,053 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:37,054 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:37,054 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m3wc36sc/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-11-26 19:35:39,534 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-11-26-19-32-08-962\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-111652037296/sagemaker-pytorch-2019-11-26-19-32-08-962/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":6,\"tied\":true}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-west-2-111652037296/sagemaker-pytorch-2019-11-26-19-32-08-962/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-11-26-19-32-08-962\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-111652037296/sagemaker-pytorch-2019-11-26-19-32-08-962/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 6 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\u001b[0m\n",
      "\u001b[31mLoad data\u001b[0m\n",
      "\u001b[31mBatchify dataset\u001b[0m\n",
      "\u001b[31mBuild the model\u001b[0m\n",
      "\u001b[31mStarting training.\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 91.91 | loss  8.33 | ppl  4136.34\u001b[0m\n",
      "\u001b[31m| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 91.50 | loss  7.38 | ppl  1606.61\u001b[0m\n",
      "\u001b[31m| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 92.49 | loss  6.87 | ppl   964.80\u001b[0m\n",
      "\u001b[31m| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 92.72 | loss  6.68 | ppl   794.86\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.36 | loss  6.45 | ppl   632.29\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 91.57 | loss  6.39 | ppl   595.52\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 91.80 | loss  6.31 | ppl   548.38\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 91.90 | loss  6.29 | ppl   539.20\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 91.97 | loss  6.08 | ppl   436.73\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 91.98 | loss  6.05 | ppl   422.07\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 91.90 | loss  5.92 | ppl   371.01\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 91.60 | loss  5.96 | ppl   388.87\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 91.39 | loss  5.94 | ppl   379.04\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 91.51 | loss  5.83 | ppl   340.01\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 283.75s | valid loss  6.03 | valid ppl   416.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 1, 'lr': 20, 'val_loss': 6.030891246884169, 'val_ppl': 416.0856992612763}\u001b[0m\n",
      "\u001b[31m| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 92.09 | loss  5.83 | ppl   341.53\u001b[0m\n",
      "\u001b[31m| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 91.76 | loss  5.80 | ppl   330.97\u001b[0m\n",
      "\u001b[31m| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 91.90 | loss  5.60 | ppl   271.56\u001b[0m\n",
      "\u001b[31m| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 91.66 | loss  5.62 | ppl   277.13\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 91.98 | loss  5.52 | ppl   249.90\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 91.93 | loss  5.55 | ppl   257.81\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.05 | loss  5.59 | ppl   268.73\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 91.81 | loss  5.65 | ppl   284.16\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.02 | loss  5.48 | ppl   239.59\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 91.95 | loss  5.49 | ppl   241.56\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 91.98 | loss  5.39 | ppl   219.18\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 91.97 | loss  5.45 | ppl   233.92\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.22 | loss  5.46 | ppl   236.21\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.12 | loss  5.38 | ppl   217.57\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 284.04s | valid loss  5.75 | valid ppl   314.53\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 2, 'lr': 20, 'val_loss': 5.751095302971249, 'val_ppl': 314.53498278374815}\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 92.36 | loss  5.45 | ppl   232.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 92.03 | loss  5.45 | ppl   233.06\u001b[0m\n",
      "\u001b[31m| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 92.02 | loss  5.25 | ppl   191.17\u001b[0m\n",
      "\u001b[31m| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 92.09 | loss  5.30 | ppl   200.75\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 91.82 | loss  5.22 | ppl   185.25\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.00 | loss  5.27 | ppl   193.69\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.07 | loss  5.33 | ppl   207.32\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.12 | loss  5.40 | ppl   221.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.11 | loss  5.23 | ppl   187.49\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.12 | loss  5.26 | ppl   192.34\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.13 | loss  5.16 | ppl   173.74\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.09 | loss  5.23 | ppl   186.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.07 | loss  5.25 | ppl   190.77\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.18 | loss  5.18 | ppl   177.47\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 284.40s | valid loss  5.65 | valid ppl   283.93\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 3, 'lr': 20, 'val_loss': 5.648728454820682, 'val_ppl': 283.9302061087303}\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 92.42 | loss  5.25 | ppl   190.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 92.17 | loss  5.27 | ppl   193.59\u001b[0m\n",
      "\u001b[31m| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 92.37 | loss  5.06 | ppl   157.97\u001b[0m\n",
      "\u001b[31m| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 92.05 | loss  5.13 | ppl   168.75\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.24 | loss  5.05 | ppl   156.66\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.25 | loss  5.11 | ppl   165.33\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.09 | loss  5.18 | ppl   176.89\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.21 | loss  5.24 | ppl   189.53\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.42 | loss  5.09 | ppl   162.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.58 | loss  5.12 | ppl   167.23\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.48 | loss  5.02 | ppl   151.02\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.46 | loss  5.09 | ppl   162.49\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.28 | loss  5.12 | ppl   166.55\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.53 | loss  5.05 | ppl   155.38\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 285.04s | valid loss  5.60 | valid ppl   270.80\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 4, 'lr': 20, 'val_loss': 5.601393558680628, 'val_ppl': 270.803525200536}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 92.90 | loss  5.12 | ppl   167.99\u001b[0m\n",
      "\u001b[31m| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 92.54 | loss  5.14 | ppl   170.05\u001b[0m\n",
      "\u001b[31m| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 92.54 | loss  4.94 | ppl   139.46\u001b[0m\n",
      "\u001b[31m| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 92.22 | loss  5.00 | ppl   149.02\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.02 | loss  4.95 | ppl   140.72\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.10 | loss  5.00 | ppl   148.11\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 91.42 | loss  5.07 | ppl   158.69\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 91.61 | loss  5.14 | ppl   170.79\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 91.90 | loss  4.98 | ppl   145.98\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 91.48 | loss  5.02 | ppl   151.35\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.00 | loss  4.91 | ppl   135.80\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 91.93 | loss  4.98 | ppl   145.65\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 91.25 | loss  5.01 | ppl   149.46\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.14 | loss  4.95 | ppl   140.62\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 284.11s | valid loss  5.58 | valid ppl   265.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 5, 'lr': 20, 'val_loss': 5.582009914122485, 'val_ppl': 265.60491272120066}\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 92.81 | loss  5.02 | ppl   151.79\u001b[0m\n",
      "\u001b[31m| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 92.56 | loss  5.04 | ppl   153.73\u001b[0m\n",
      "\u001b[31m| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 92.86 | loss  4.84 | ppl   126.57\u001b[0m\n",
      "\u001b[31m| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 92.62 | loss  4.91 | ppl   135.96\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 91.72 | loss  4.86 | ppl   129.46\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.30 | loss  4.92 | ppl   136.79\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.46 | loss  4.98 | ppl   145.84\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.06 | loss  5.05 | ppl   156.77\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.49 | loss  4.91 | ppl   135.50\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.70 | loss  4.94 | ppl   139.67\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 91.84 | loss  4.84 | ppl   125.85\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.55 | loss  4.90 | ppl   134.81\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.59 | loss  4.93 | ppl   138.93\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.62 | loss  4.88 | ppl   131.10\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 285.41s | valid loss  5.56 | valid ppl   258.59\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 6, 'lr': 20, 'val_loss': 5.555247321546878, 'val_ppl': 258.5909117167824}\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  5.57 | test ppl   262.91\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m2019-11-26 20:04:53,314 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-11-26 20:05:50 Uploading - Uploading generated training model\n",
      "2019-11-26 20:06:10 Completed - Training job completed\n",
      "Billable seconds: 1888\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Hosting script\n",
    "We are going to provide custom implementation of `model_fn`, `input_fn`, `output_fn` and `predict_fn` hosting functions in a separate file, which is in the same Git repo as the training script: \n",
    "[https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py](https://github.com/awslabs/amazon-sagemaker-examples/blob/training-scripts/pytorch-rnn-scripts/generate.py). \n",
    "We will use Git integration for hosting too since the hosting code is also in the Git repo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also put your training and hosting code in the same file but you would need to add a main guard (`if __name__=='__main__':`) for the training code, so that the container does not inadvertently run it at the wrong point in execution during hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "The PyTorch model uses a npy serializer and deserializer by default. For this example, since we have a custom implementation of all the hosting functions and plan on using JSON instead, we need a predictor that can serialize and deserialize JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n",
    "\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since hosting functions implemented outside of train script we can't just use estimator object to deploy the model. Instead we need to create a PyTorchModel object using the latest training job to get the S3 location of the trained model data. Besides model data location in S3, we also need to configure PyTorchModel with the script and source directory (because our `generate` script requires model and data classes from source directory), an IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "desc = sagemaker_session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "trained_model_location = desc['ModelArtifacts']['S3ModelArtifacts']\n",
    "model = PyTorchModel(model_data=trained_model_location,\n",
    "                     role=role,\n",
    "                     framework_version='1.0.0',\n",
    "                     entry_point='generate.py',\n",
    "                     source_dir='pytorch-rnn-scripts',\n",
    "                     predictor_cls=JSONPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint and we are going to use the `sagemaker.pytorch.model.PyTorchModel.deploy` method to do this. We can use a CPU-based instance for inference (in this case an ml.m4.xlarge), even though we trained on GPU instances, because at the end of training we moved model to cpu before returning it. This way we can load trained model on any device and then move to GPU if CUDA is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We are going to use our deployed model to generate text by providing random seed, temperature (higher will increase diversity) and number of words we would like to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422nd wetland Potro integrating Eighty Demand lanyard sanctified Cévennes agendas detonations apology\n",
      "Escorial DeMille Charlynch helix Zeffirelli Victorien Hessian PRG imitators sheath barbershops Suresnes\n",
      "classifications roaming Alford RPGs guts Marshall Godrich Briefing Branislav 560 Roscoe souls\n",
      "beheaded Borough AIL Shoko Cowper Potworowski goalkeepers Kenith Roumette Ortona Ismailia S.259\n",
      "Wreath partied Bag survived applications annealed fisherman Plains luncheon Đình Tokyo heckles\n",
      "egos Dobson Aag HevyDevy Investigates able Hoodoo Hendrie quatre Needled Prussian visitor\n",
      "Hashomer Redfearn Matteino Stjepan Yiwu Bree roof Surdas simpler Measure bey 411\n",
      "Babang widened Altrock Kulukundis Maverick Wing startling Kostrzewska Featley emails yes Odetta\n",
      "Venture grammars Nachtigall Franka silence Kommunistblad 837 emcee 562 hOWLetts deducted reproductive\n",
      "traveller difficult Rogatti 90s Casino Tracey Screen sabotaging Udell abdicated anglica College\n",
      "lengthy Yuji Nonsense Forth realistic Ioke Quitman repairs Isonomy Costanza Eternia wingers\n",
      "moneyed interferometer impoverishing medicine sentimentality legume 239Pu turning obtains Beautiful Li Locks\n",
      "Sackheim stimmi bombardment flashy hibernate millinery Prover Celeste memoir declination Venu pectoral\n",
      "bright inamyloidy Silverman 549th liability Sangidu chimes Nye Pursey Katalog Gowri morphs\n",
      "condense Tecomán bipinnate Gilmore Marburg Sahel Bains allure spatializer Pogi dole Spence\n",
      "achieving satirist tripped clarity electrocyclizations Roussel Megabus frustrating lagena paratransgenesis neater Forgive\n",
      "1650 Yanow lethal rabbit jolly enactments inventory callback \n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    'seed': 200,\n",
    "    'temperature': 510.0,\n",
    "    'words': 200\n",
    "}\n",
    "response = predictor.predict(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
