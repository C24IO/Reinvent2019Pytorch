{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting text models: IMDB sentiment analysis\n",
    "\n",
    "Captum (“comprehension” in Latin) is an open source, extensible library for model interpretability built on PyTorch. The primary audiences for Captum are model developers who are looking to improve their models and understand which features are important and interpretability researchers focused on identifying algorithms that can better interpret many types of models.\n",
    "\n",
    "#### Sources: \n",
    "* https://captum.ai/tutorials/IMDB_TorchText_Interpret\n",
    "* https://captum.ai/docs/introduction.html\n",
    "* https://captum.ai/docs/extension/integrated_gradients\n",
    "* https://arxiv.org/pdf/1703.01365.pdf\n",
    "* https://towardsdatascience.com/interpretable-neural-networks-45ac8aa91411\n",
    "* https://github.com/ankurtaly/Integrated-Gradients\n",
    "* http://theory.stanford.edu/~ataly/Talks/sri_attribution_talk_jun_2017.pdf\n",
    "* https://medium.com/@kartikeyabhardwaj98/integrated-gradients-for-deep-neural-networks-c114e3968eae\n",
    "* http://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html\n",
    "* https://psturmfels.github.io/VisualizingExpectedGradients/\n",
    "* https://www.youtube.com/watch?v=iVSIFm0UN9I\n",
    "* https://medium.com/@SeoJaeDuk/archived-post-axiomatic-attribution-for-deep-networks-b4af79d5ed32\n",
    "* http://pages.cs.wisc.edu/~jiefeng/docs/neurips2019/poster.pdf\n",
    "* http://pages.cs.wisc.edu/~jiefeng/docs/neurips2019/slides.pdf\n",
    "* https://vimeo.com/238242575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get ready to execute this on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install torchtext nvidia-ml-py3 captum\n",
    "pip install -U spacy torch msgpack\n",
    "python -m spacy download en\n",
    "python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import InterpretableEmbeddingBase, TokenReferenceBase\n",
    "from captum.attr import visualization\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -cv https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!mkdir data models\n",
    "!(cd data ; wget -cv https://ar51.s3.amazonaws.com/aclImdb_v1.tar.gz)\n",
    "!(cd data ; tar -xzf aclImdb_v1.tar.gz)\n",
    "!(cd models ; wget -cv https://ar51.s3.amazonaws.com/imdb-model-cnn.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        #text = text.permute(1, 0)\n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('models/imdb-model-cnn.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_sigmoid(input):\n",
    "    return torch.sigmoid(model(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower=True, tokenize='spacy')\n",
    "Label = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = torchtext.datasets.IMDB.splits(text_field=TEXT,\n",
    "                                      label_field=Label,\n",
    "                                      train='train',\n",
    "                                      test='test',\n",
    "                                      path='data/aclImdb')\n",
    "test, _ = test.split(split_ratio = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -cv http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!(cd data ; wget -cv https://ar51.s3.amazonaws.com/glove.6B.50d.txt.tar.gz)\n",
    "!(cd data ; tar -xzf glove.6B.50d.txt.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab\n",
    "\n",
    "#loaded_vectors = vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "# If you prefer to use pre-downloaded glove vectors, you can load them with the following two command line\n",
    "loaded_vectors = torchtext.vocab.Vectors('data/glove.6B.50d.txt')\n",
    "TEXT.build_vocab(train, vectors=loaded_vectors, max_size=len(loaded_vectors.stoi))\n",
    "    \n",
    "TEXT.vocab.set_vectors(stoi=loaded_vectors.stoi, vectors=loaded_vectors.vectors, dim=loaded_vectors.dim)\n",
    "Label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary Size: ', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IND = TEXT.vocab.stoi['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model, sentence, min_len = 7, label = 0):\n",
    "    model.eval()\n",
    "    text = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(text) < min_len:\n",
    "        text += ['pad'] * (min_len - len(text))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in text]\n",
    "\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    input_indices = torch.LongTensor(indexed)\n",
    "    input_indices = input_indices.unsqueeze(0)\n",
    "    \n",
    "    # input_indices dim: [sequence_length]\n",
    "    seq_length = min_len\n",
    "\n",
    "    # pre-computing word embeddings\n",
    "    input_embedding = interpretable_embedding.indices_to_embeddings(input_indices)\n",
    "\n",
    "    # predict\n",
    "    pred = forward_with_sigmoid(input_embedding).item()\n",
    "    pred_ind = round(pred)\n",
    "\n",
    "    # generate reference for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "    reference_embedding = interpretable_embedding.indices_to_embeddings(reference_indices)\n",
    "\n",
    "    # compute attributions and approximation delta using integrated gradients\n",
    "    attributions_ig, delta = ig.attribute(input_embedding, reference_embedding, n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', Label.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            Label.vocab.itos[pred_ind],\n",
    "                            Label.vocab.itos[label],\n",
    "                            Label.vocab.itos[1],\n",
    "                            attributions.sum(),       \n",
    "                            text[:len(attributions)],\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_sentence(model, 'It was a fantastic performance !', label=1)\n",
    "interpret_sentence(model, 'Best film ever', label=1)\n",
    "interpret_sentence(model, 'Such a great show!', label=1)\n",
    "interpret_sentence(model, 'It was a horrible movie', label=0)\n",
    "interpret_sentence(model, 'I\\'ve never watched something as bad', label=0)\n",
    "interpret_sentence(model, 'It is a disgusting movie!', label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visualize attributions based on Integrated Gradients')\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data_records_ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/sentiment_analysis.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
