{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZM-JDrE4tN1"
   },
   "source": [
    "# torch.optim: optimization package in PyTorch\n",
    "\n",
    "In the following notebook, we shall take a closer look at the `torch.optim` package.\n",
    "\n",
    "The reference documentation for the package is [here](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "The package typically provides gradient-descent optimization algorithms.\n",
    "\n",
    "Currently, the following algorithms are available:\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "  - with heavy-ball and nesterov momentum variants (optional)\n",
    "  - with L2 weight decay (optional)\n",
    "- Adadelta\n",
    "- Adagrad\n",
    "- Adam\n",
    "- SparseAdam\n",
    "- Adamax\n",
    "- Averaged SGD\n",
    "- L-BFGS\n",
    "- RMSProp\n",
    "- RProp (resilient backprop)\n",
    "\n",
    "The optim package is available via `import torch.optim as optim`\n",
    "\n",
    "It makes no assumption on the function it is optimizing, and treats functions as a blackbox `f(x)`.\n",
    "\n",
    "The package is designed to work well with neural networks defined using  `torch.nn.Module`, but does not rely on `torch.nn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HkWHjSQv5JlI"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('If you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.')\n",
    "else:\n",
    "    print('GPU ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GbqLHgUWRHze"
   },
   "source": [
    "## A walkthrough of the API\n",
    "\n",
    "The optim package has optimizers placed in two buckets:\n",
    "\n",
    "1. The optimizer evaluates f(x) only once\n",
    "    - examples: SGD, RMSProp, RProp, Adagrad, Adam, etc.\n",
    "1. The optimizer evaluates f(x) multiple times, with different inputs\n",
    "    - example: L-BFGS, evolutionary algorithms\n",
    "\n",
    "\n",
    "### Case 1: Evaluate-once optimizers\n",
    "\n",
    "In the case of (1), when f(x) is evaluated only once, we provide a simple interface that is roughly as follows:\n",
    "\n",
    "```\n",
    "y = f(x) # f is the function we are optimizing\n",
    "x        # params to optimize. a Tensor or list of Tensors with requires_grad=True\n",
    "\n",
    "# construct an optimizer object\n",
    "optimizer = optim.SGD(x, lr=0.1, momentum=0.9)\n",
    "\n",
    "# one gradient-descent step\n",
    "optimizer.zero_grad()\n",
    "y = f(x)\n",
    "y.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "Here's an example of how these optimizers will be used with `nn.Module` based neural networks:\n",
    "\n",
    "```\n",
    "model = torchvision.models.resnet18()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "for inputs, labels in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### Case 2: Optimizer does multiple evaluations of f(x)\n",
    "\n",
    "In this case the API is a little more involved. One has to define the function `f(x)` as a python closure, and give that to `optimizer.step`.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "for input, target in dataset:\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "```\n",
    "\n",
    "## Classifying two-moons dataset using Logistic Regression\n",
    "\n",
    "Let us work through a small example of classifying the noisy two-moons dataset generated via scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Bc7xMONi6JJc"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(100, shuffle=True, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "aAOTLwAJ74Ce",
    "outputId": "dd089fef-eaaa-4f61-e945-ba75abe4b78f"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtiJwD-vRnyC"
   },
   "source": [
    "The goal is to build a classifier that classifies the white points from the black ones.\n",
    "\n",
    "Let us create weights and biases for a model `y = sigmoid(W * X + b)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fESxbldq9GOy"
   },
   "outputs": [],
   "source": [
    "weight = torch.randn(2, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32) # convert from numpy to PyTorch format\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTOEsYEVSlyr"
   },
   "source": [
    "Now, let's create an Stochastic Gradient Descent optimizer that will optimize the weights and biases of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "E5eqxh9wAKgL"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([weight, bias], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iakcZKoNS26B"
   },
   "source": [
    "We shall use the Binary Cross Entropy loss, which implicitly will do the Sigmoid operation to convert the inputs to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pyMVxlCrBICE"
   },
   "outputs": [],
   "source": [
    "loss_fn = F.binary_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iowMShh3TCpn"
   },
   "source": [
    "Now, let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5595,
     "status": "ok",
     "timestamp": 1529905103810,
     "user": {
      "displayName": "Soumith Chintala",
      "photoUrl": "//lh5.googleusercontent.com/-ASzCTwFtgW0/AAAAAAAAAAI/AAAAAAAASL4/OWJyxUEEJ64/s50-c-k-no/photo.jpg",
      "userId": "106447253626219410322"
     },
     "user_tz": 240
    },
    "id": "DXNRgUW9B7Ct",
    "outputId": "8c5f7c60-81a0-4876-85a1-7bb80e93f879"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "loss_epoch = [0] * n_epochs\n",
    "\n",
    "for epoch in range(n_epochs): # number of epochs through the dataset\n",
    "  for i in range(100): # size of dataset\n",
    "    optimizer.zero_grad()\n",
    "    y_predicted = torch.dot(X[i], weight) + bias\n",
    "    loss = loss_fn(y_predicted[0], y[i])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    loss_epoch[epoch] += loss.item()\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxD65EV2XgFJ"
   },
   "source": [
    "An interesting exercise would be to see how different optimization algorithms fare in terms of rate of convergence.\n",
    "\n",
    "Let us construct a few, and plot their losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pL6hkbfHYJce"
   },
   "source": [
    "Now, let's construct a new model that is cloned 4 times, along with these optimizers which take each of the 4 cloned models:\n",
    "\n",
    "- Plain SGD\n",
    "- SGD with momentum\n",
    "- RMSProp\n",
    "- Adam\n",
    "\n",
    "We clone the model so that all optimizations can start from the same starting weights, for equal comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iMsnXxT4YK9N"
   },
   "outputs": [],
   "source": [
    "weight = torch.randn(2, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "params_sgd = [weight.data.clone().requires_grad_(), bias.data.clone().requires_grad_()]\n",
    "sgd = optim.SGD(params_sgd, lr=0.01)\n",
    "\n",
    "params_sgdmom = [weight.data.clone().requires_grad_(), bias.data.clone().requires_grad_()]\n",
    "sgdmom = optim.SGD(params_sgdmom, lr=0.01, momentum=0.5)\n",
    "\n",
    "params_rms = [weight.data.clone().requires_grad_(), bias.data.clone().requires_grad_()]\n",
    "rms = optim.RMSprop(params_rms, lr=0.01)\n",
    "\n",
    "params_adam = [weight.data.clone().requires_grad_(), bias.data.clone().requires_grad_()]\n",
    "adam = optim.Adam(params_adam, lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HFnHZMFZqU9"
   },
   "source": [
    "Next, let's make a function of the training loop, so that we can re-use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-zzgL0cJUZVQ"
   },
   "outputs": [],
   "source": [
    "def train(optimizer, weight, bias):\n",
    "  loss_epoch = [0] * n_epochs\n",
    "\n",
    "  for epoch in range(n_epochs): # number of epochs through the dataset\n",
    "    for i in range(100): # size of dataset\n",
    "      optimizer.zero_grad()\n",
    "      y_predicted = torch.dot(X[i], weight) + bias\n",
    "      loss = loss_fn(y_predicted[0], y[i])\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "  \n",
    "      loss_epoch[epoch] += loss.item()\n",
    "  return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KBIC42_QY3F5"
   },
   "outputs": [],
   "source": [
    "loss_sgd = train(sgd, params_sgd[0], params_sgd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EE9PYPKCaii2"
   },
   "outputs": [],
   "source": [
    "loss_sgdmom = train(sgdmom, params_sgdmom[0], params_sgdmom[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Fh0o4DnlbBlm"
   },
   "outputs": [],
   "source": [
    "loss_rms = train(rms, params_rms[0], params_rms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "V5mum3ofbCCK"
   },
   "outputs": [],
   "source": [
    "loss_adam = train(adam, params_adam[0], params_adam[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "9mupQH6uameb",
    "outputId": "35dd96e7-c1a1-417c-86af-ea25806d0297"
   },
   "outputs": [],
   "source": [
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "legend_sgd, = plt.plot(loss_sgd, label='sgd')\n",
    "legend_sgdmom, = plt.plot(loss_sgdmom, label='sgd with momentum')\n",
    "legend_rms, = plt.plot(loss_rms, label='rmsprop')\n",
    "legend_adam, = plt.plot(loss_adam, label='adam')\n",
    "plt.legend(handles=[legend_sgd, legend_sgdmom, legend_rms, legend_adam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KfGaiSkcUg2"
   },
   "source": [
    "## Exercise1: play with the hyperparameters of the optimizers and make training convergence faster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VrjtEv7Wc7bN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg86x9loeDs_"
   },
   "source": [
    "## Exercise2: train on the MNIST dataset\n",
    "\n",
    "The MNIST dataset is available via the following code snippet. Build a small logistic regression model that trains on it using different optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ezE7x5U-c9RU"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "R8LbsqlFeqmx"
   },
   "outputs": [],
   "source": [
    "print(\"Size of MNIST training set: \", len(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Uxn19ivve0vn"
   },
   "outputs": [],
   "source": [
    "# Example of getting a particular sample:\n",
    "data, label = mnist_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tmdXYzzAfEGj"
   },
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(label.item()) # label is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0rlPXIcpfG6_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "optimization_tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
