{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YE7kqZhLOeo"
   },
   "source": [
    "# Visual understanding of Convnets\n",
    "\n",
    "Up to now, you have seen how to use neural networks to perform a number of tasks, from discriminative tasks such as classification to generative tasks.\n",
    "\n",
    "But what have the network done internally remains a mystery to us. Can we understand a bit better what each layer is doing?\n",
    "\n",
    "Roughly, the first layers of the CNN correspond to low level feature extractors, such as edge and corner detectors.\n",
    "Intermediate layers takes the detectors from the early layers and are able to extract more complex patterns, such as parts of objects.\n",
    "Finally, later laters are able to represent more complex combinations of parts of objects and textures.\n",
    "\n",
    "An example visualization is as follows\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*Ji5QhY9QXBlpNNLH4qAcNA.png)\n",
    "\n",
    "\n",
    "## Exploring intermediate activations in a pretrained CNN\n",
    "\n",
    "We will start with a model pre-trained on ImageNet, and will inspect what each layer is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pVV1L_mqdfL4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "if not torch.cuda.is_available():\n",
    "    print('If you are running this notebook in Colab, go to the Runtime menu and select \"Change runtime type\" to switch to GPU.')\n",
    "else:\n",
    "    print('GPU ready to go!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaWZEVuXMs-o"
   },
   "source": [
    "#### Helper functions for visualization\n",
    "\n",
    "Let's now implement a few helper functions that will be useful for downloading images and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q2GH0wWOsUhf"
   },
   "outputs": [],
   "source": [
    "def load(url):\n",
    "    \"\"\"\n",
    "    Given an url of an image, downloads the image and\n",
    "    returns a PIL image\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    Convenience wrapper around plt.imshow\n",
    "    \"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = np.array(img)\n",
    "    elif isinstance(img, torch.Tensor):\n",
    "        img = img.detach().cpu().numpy()\n",
    "    if img.ndim == 2:\n",
    "        plt.imshow(img, cmap=plt.get_cmap('jet'), interpolation='bicubic')\n",
    "    else:\n",
    "        plt.imshow(img, interpolation='bicubic')\n",
    "    plt.axis('off')\n",
    "\n",
    "def upsample(image, size):\n",
    "  \"\"\"\n",
    "  Performs a bicubic upsampling of the image\n",
    "  \n",
    "  Arguments:\n",
    "    image: either a np.ndarray, a torch.Tensor\n",
    "        or a PIL Image\n",
    "    size: a (width, height) tuple\n",
    "  \"\"\"\n",
    "  if isinstance(image, torch.Tensor):\n",
    "      image = image.detach().cpu().numpy()\n",
    "  if isinstance(image, np.ndarray):\n",
    "      image = (image - image.min()) / (image.max() - image.min())\n",
    "      image = Image.fromarray(image)\n",
    "  image = image.resize(size, resample=Image.BICUBIC)\n",
    "  return np.array(image)\n",
    "\n",
    "def blend(image, activation, alpha=0.5):\n",
    "    \"\"\"\n",
    "    blend two images together\n",
    "    \n",
    "    Arguments:\n",
    "      image: a PIL Image\n",
    "      activation: either a np.ndarray, a torch.Tensor\n",
    "        or a PIL Image\n",
    "    \"\"\"\n",
    "    activation = upsample(activation, image.size)\n",
    "    activation = cm.jet(activation, bytes=True)\n",
    "    activation = Image.fromarray(activation).convert('RGB')\n",
    "    blended = Image.blend(image, activation, alpha)\n",
    "    return np.array(blended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nOhDhV0TNBc7"
   },
   "source": [
    "### The model\n",
    "\n",
    "We will be using a ResNet model pre-trained on ImageNet.\n",
    "Thanks to `torchvision`, it's very easy to obtain pre-trained models.\n",
    "But `torchvision` only provide models that return a classification score, so we will be\n",
    "adapting the models from `torchvision` to also return intermediate layers.\n",
    "\n",
    "Adapting a model requires understanding what are the constituent building blocks from that model.\n",
    "The implementation of all models available in `torchvision` can be found [here](https://github.com/pytorch/vision/tree/master/torchvision/models).\n",
    "\n",
    "More particularly, we can see [in those lines](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L103-L111) where a generic ResNet model implements its main building blocks, so we will be using those for our use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RlwN9tgzd-kT"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "class CustomResNet(models.resnet.ResNet):\n",
    "    \"\"\"\n",
    "    ResNet model that returns all the intermediate\n",
    "    ResNet blocks\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns a list with the results of each resnet block\n",
    "        \"\"\"\n",
    "        # adds an extra batch dimension for simplicity\n",
    "        if x.dim() == 3:\n",
    "          x = x[None]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        layers = [self.layer1, self.layer2, self.layer3, self.layer4]\n",
    "        result = []\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "            result.append(x)\n",
    "        # returns the result, removing the batch dimension\n",
    "        # for simplicity\n",
    "        \n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x).view(x.size(0), -1, 1, 1)\n",
    "        result.append(x)\n",
    "        \n",
    "        return [r[0] for r in result]\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    helper function to return a pre-trained model.\n",
    "    Follows closely the implementation from\n",
    "    torchvision\n",
    "    \"\"\"\n",
    "    model = CustomResNet(models.resnet.Bottleneck,\n",
    "                           [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(\n",
    "            models.resnet.model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # run on GPU if available - recommended!\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).to(device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).to(device)\n",
    "\n",
    "normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "\n",
    "# transformation to the input image\n",
    "# so that it can be fed to the network\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY0AJjpYOirA"
   },
   "source": [
    "Before we load up our altered version of ResNet, let's see what its architecture looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_resnet = models.ResNet(models.resnet.Bottleneck, [3, 4, 6, 3])\n",
    "print(regular_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a new model from pre-trained weights and set it to inference mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Mzl-Id-ahnp5"
   },
   "outputs": [],
   "source": [
    "# load a pretrained model and put it in evaluation mode\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "# as we are not training the model,\n",
    "# we can set its parameters to not require gradient\n",
    "for parameter in model.parameters():\n",
    "  parameter.requires_grad_(False)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtU4QgYnOo0P"
   },
   "source": [
    "### Loading images for inspection\n",
    "\n",
    "With the help of our utility functions from before, we can load an image from an arbitrary url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PIEQwhSQi98b"
   },
   "outputs": [],
   "source": [
    "image = load('http://www.zooclub.ru/attach/26000/26132.jpg')\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlu6DK-jPigv"
   },
   "source": [
    "Let's perform the data pre-processing to convert the images into something that the network can understand.\n",
    "This is basically performing the same pre-processing as what was done during the training of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yW4ejPF1k6Oj"
   },
   "outputs": [],
   "source": [
    "processed_image = img_transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oJuel2-PyBb"
   },
   "source": [
    "Now let's evaluate the model on our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oi-mZD0KnPZc"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(processed_image.to(device))\n",
    "\n",
    "print([output.shape for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mBvLr8jRIvI"
   },
   "source": [
    "### Defining how to visualize the feature maps\n",
    "\n",
    "Our model will be returning a set of feature maps that are high-dimensional images (containing possibly thousands of channels, in our example just above up to 2048 channels). How to visualize such high-dimensional images?\n",
    "\n",
    "One of the simplest things to do is to consider the norm over all the channels, and display it as a heat map. This is what we will be doing next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pXb0I8YboRL2"
   },
   "outputs": [],
   "source": [
    "# get the activations of the network for each layer, by just\n",
    "# computing the norm over the channel dimension\n",
    "activations = [output.norm(2, dim=0) for output in outputs]\n",
    "\n",
    "for activation in activations[:-1]:\n",
    "  imshow(activation)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YiVDXVHNQ-aW"
   },
   "source": [
    "Interesting!\n",
    "By looking at the first activations of the network, we see that it pays attention to small details and contours, while the higher levels contain more semantic information, like the faces of the dogs.\n",
    "\n",
    "For easier visualization, let's blend the original image with the activation from the third layer of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eOfoaAdKGhzY"
   },
   "outputs": [],
   "source": [
    "imshow(blend(image, activations[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfhxDr3sSa1R"
   },
   "source": [
    "Nice! It looks like it's really focusing on the dogs faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__5IksNgS2nt"
   },
   "source": [
    "### Exercise\n",
    "We have tried a very simple transformation on the feature maps (the norm over the channels) to inspect what the network was focusing on.\n",
    "Try out other possible transformations and see if we can extract more information from the CNN.\n",
    "\n",
    "**Hint:** Each channel of the feature map alone also contains valuable information. Maybe try visualizing what each channel in the feature map shows?\n",
    "\n",
    "**Hint:** If you want to visualize a lot of images, it might be useful to have them displayed as a grid of images. For that, you can use [`torchvision.utils.make_grid`](https://pytorch.org/docs/stable/torchvision/utils.html#torchvision.utils.make_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Y26f4olxUEZP"
   },
   "outputs": [],
   "source": [
    "# you can experiment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrQA3aUcUB_8"
   },
   "source": [
    "## Exploring the gradients with respect to each layer\n",
    "\n",
    "Up to now we have only used the activations of a specific layer.\n",
    "But another possibility, which is very easy to do with PyTorch, is to leverage the gradients with respect to a specific layer of a network.\n",
    "By backpropagating until the input image, it is possible to perform gradient descent on the image space.\n",
    "\n",
    "Let's start with a new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n3P9xsTvilZV"
   },
   "outputs": [],
   "source": [
    "img = load('https://tsicloud.com/wp-content/uploads/2017/06/cloud.jpg')\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp9gBjpXi1hJ"
   },
   "source": [
    "What can the network see in this image, if we push it a bit?\n",
    "\n",
    "Let's take the activations of the network, and modify the original image such that we magnify the original activations of the network.\n",
    "\n",
    "This will be achieved by using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v6RKAnIunZ7K"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "shift = 20\n",
    "do_random_shifts = True\n",
    "\n",
    "def random_shift(data):\n",
    "  return data[:, random.randint(0, shift):-random.randint(1, shift),\n",
    "             random.randint(0, shift):-random.randint(1, shift)]\n",
    "\n",
    "# by default, we will be using the 3rd layer to compute the\n",
    "# gradients\n",
    "layer = 2\n",
    "def default_grad_fn(output):\n",
    "    output[layer].backward(output[layer])\n",
    "\n",
    "def dream(img, n_iter=100,\n",
    "          grad_fn=default_grad_fn,\n",
    "          learning_rate=0.02):\n",
    "    \"\"\"\n",
    "    This function will take an image img and perform a\n",
    "    few steps of gradient descent on the image space so\n",
    "    that we magnify the intermediate activations of the\n",
    "    image.\n",
    "    \n",
    "    Arguments:\n",
    "      img: a PIL.Image\n",
    "      n_iter (int): number of iterations of SGD to perform\n",
    "      grad_fn (function): the function that we will use\n",
    "        to compute the gradients\n",
    "      learning_rate (int): the learning rate for the SGD\n",
    "      \n",
    "    \"\"\"\n",
    "  \n",
    "    # constants that represent the minimum and maximum\n",
    "    # value that can be represented in the processed\n",
    "    # image space\n",
    "    clamp_min = -mean[:, None, None] / std[:, None, None]\n",
    "    clamp_max = (1 - mean[:, None, None]) / std[:, None, None]\n",
    "    clamp_min = clamp_min.to(device)\n",
    "    clamp_max = clamp_max.to(device)\n",
    "\n",
    "    # pre-process image\n",
    "    processed_image = img_transform(img)\n",
    "    data = processed_image.to(device)\n",
    "    # perform a few steps of SGD\n",
    "    for i in range(n_iter):\n",
    "      # we want to have the gradients wrt the image\n",
    "      data.requires_grad_()\n",
    "      x = data\n",
    "      if do_random_shifts:\n",
    "        x = random_shift(data)\n",
    "      output = model(x)\n",
    "      grad_fn(output)\n",
    "      # perform the gradient update in the image\n",
    "      with torch.no_grad():\n",
    "          # get the mean of the abs of the gradient\n",
    "          # so that we can have steps which take its\n",
    "          # magnitude into account\n",
    "          grad_mean = data.grad.abs().mean()\n",
    "          # do a simple SGD step\n",
    "          data += data.grad * learning_rate / grad_mean\n",
    "          # clip the results so that they are not out of the\n",
    "          # image range\n",
    "          data = torch.max(data, clamp_min)\n",
    "          data = torch.min(data, clamp_max)\n",
    "    # put image back in the original representation\n",
    "    # before normalization and also\n",
    "    # clip image to be between 0 and 1\n",
    "    result = unnormalize(data.to('cpu')).clamp(0, 1)\n",
    "    # permute image from CxHxW to HxWxC\n",
    "    return result.permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ziV44gymY6Ti"
   },
   "outputs": [],
   "source": [
    "result = dream(img)\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-rvd0M3ntmB"
   },
   "source": [
    "Woah, the cloud got a fish-like skin!\n",
    "\n",
    "#### Exercise\n",
    "Try modifying the number of iterations, the learning rate and the layer to see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oV7b8lHgoZWS"
   },
   "outputs": [],
   "source": [
    "# your modifications here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsJEMecloch9"
   },
   "source": [
    "### Other gradient functions\n",
    "\n",
    "In the previous exercise, you have tried a few modifications to the gradient function.\n",
    "\n",
    "But can we control even more what kinds of transformation we want the CNN to apply.\n",
    "\n",
    "For example. what if we want the network to put more emphasis on the features that are\n",
    "activated from another image?\n",
    "\n",
    "Let's recompute the activations from the first image, and use it as a guide for what we would\n",
    "like the features of the new image to look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NNs8NtejrzEK"
   },
   "outputs": [],
   "source": [
    "# re-load the image of the dogs and compute the features\n",
    "image = load('http://www.zooclub.ru/attach/26000/26132.jpg')\n",
    "processed_image = img_transform(image)\n",
    "data = processed_image.to(device)\n",
    "guide = model(data)\n",
    "\n",
    "# define a new gradient update function\n",
    "# that takes the features from another \n",
    "# image as a guide\n",
    "def my_grad_fn(output):\n",
    "  # let's use the result from the 3rd layer\n",
    "  layer = 2\n",
    "  o = output[layer]\n",
    "  g = guide[layer]\n",
    "  shape = o.shape\n",
    "  o = o.view(o.shape[0], -1)\n",
    "  g = g.view(g.shape[0], -1)\n",
    "  # compute the dot product (similarity)\n",
    "  # of the features for each spacial location\n",
    "  r = torch.matmul(o.t(), g)\n",
    "  # and take the max feature activation\n",
    "  # per spatial location\n",
    "  r = g[:, r.argmax(1)]\n",
    "  r = r.view(shape)\n",
    "  # use it as the gradient for the layer\n",
    "  output[layer].backward(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A4Y6M0Ujle3k"
   },
   "outputs": [],
   "source": [
    "result = dream(img, grad_fn=my_grad_fn)\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xlfAWmT2kxJ"
   },
   "source": [
    "I can see dogs in the clouds!\n",
    "\n",
    "### Exercise\n",
    "Try out different layers and images.\n",
    "Also, maybe try out different gradient functions and see if you can enforce other constraints in to appear in your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TPHD9N-ZkxbQ"
   },
   "outputs": [],
   "source": [
    "# you can experiment here.\n",
    "# this is an example to get you started\n",
    "\n",
    "def my_grad_fn2(output):\n",
    "    grad_ = torch.zeros(1000, 1, 1, device=device)\n",
    "    # use the class correspondences from ImageNet\n",
    "    # which can be found in https://github.com/torch/tutorials/blob/master/7_imagenet_classification/synset_words.txt\n",
    "    # to set the focus on dogs and cats\n",
    "    grad_[153] = 1\n",
    "    grad_[281] = 1\n",
    "    output[4].backward(grad_)\n",
    "\n",
    "    \n",
    "result = dream(img, grad_fn=my_grad_fn2)\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MQDB5La3vBM"
   },
   "source": [
    "That's it.\n",
    "\n",
    "With this notebook, you hopefully got a bit better understanding on how to perform operations with gradients in PyTorch,\n",
    "and also what the activations of internal layers of CNNs look like."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "visual_understanding_convnets.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
