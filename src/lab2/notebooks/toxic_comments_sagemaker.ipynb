{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Text Classification using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been sourced from the following blogs by Kaushal Trivedi [1](https://medium.com/huggingface/introducing-fastbert-a-simple-deep-learning-library-for-bert-models-89ff763ad384) [2](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d) and the associated [GitHub repos](https://github.com/kaushaltrivedi/fast-bert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understand whats happening here - this is the way we are using SageMaker to fine tune Hugging Face BERT models\n",
    "\n",
    "## Anatomy of a typical Amazon SageMaker container \n",
    "\n",
    "![SageMaker Architecture](../img/sagemaker-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle components of the architecture\n",
    "\n",
    "*Container* - we start off this lab by building our own container, and using SageMaker Service to train it and deploy the resultant model. I have commented it out because as of Nov 28 2019 the resultant container cannot train properly due to an unmet dependancy. As of this writing I am still debugging it. It takes around 22 mins of clock time to build this container and push it to ECR, from scratch on ml.p2.xlarge.\n",
    "\n",
    "Once the container is ready we proceed with the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!../container/build_and_push.sh\n",
    "#We have prebuilt containers and made them available to be pulled in us-east-1 and us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets talk about the container a bit - \n",
    "\n",
    "[This notebook is a bit lean](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/scikit_bring_your_own) - because most of our code resides in the container - this notebook is sort of an orchastrator - \n",
    "\n",
    "```bash\n",
    ".\n",
    "├── container\n",
    "│   ├── bert\n",
    "│   │   ├── download_pretrained_models.py\n",
    "│   │   ├── nginx.conf\n",
    "│   │   ├── predictor.py\n",
    "│   │   ├── serve\n",
    "│   │   ├── train\n",
    "│   │   └── wsgi.py\n",
    "│   ├── build_and_push.sh\n",
    "│   └── Dockerfile_gpu\n",
    "```\n",
    "\n",
    "We have 3 important components in this container - \n",
    "\n",
    "1. __[nginx][nginx]__ is a light-weight layer that handles the incoming HTTP requests and manages the I/O in and out of the container efficiently.\n",
    "2. __[gunicorn][gunicorn]__ is a WSGI pre-forking worker server that runs multiple copies of your application and load balances between them.\n",
    "3. __[flask][flask]__ is a simple web framework used in the inference app that you write. It lets you respond to call on the `/ping` and `/invocations` endpoints without having to write much \n",
    "Lets talk about each file in turn - \n",
    "\n",
    "* __Dockerfile__: The _Dockerfile_ describes how the image is built and what it contains. It is a recipe for your container and gives you tremendous flexibility to construct almost any execution environment you can imagine. Here. we use the Dockerfile to describe a pretty standard python science stack and the simple scripts that we're going to add to it. See the [Dockerfile reference][dockerfile] for what's possible here.\n",
    "\n",
    "* __build\\_and\\_push.sh__: The script to build the Docker image (using the Dockerfile above) and push it to the [Amazon EC2 Container Registry (ECR)][ecr] so that it can be deployed to SageMaker. Specify the name of the image as the argument to this script. The script will generate a full name for the repository in your account and your configured AWS region. If this ECR repository doesn't exist, the script will create it.\n",
    "\n",
    "\n",
    "* __download_pretrained_models.py__\n",
    "    Going to download the pre-trained BERT models from Hugging Face's repo\n",
    "    \n",
    "* __train__: The main program for training the model. When you build your own algorithm, you'll edit this to include your training code.\n",
    "* __serve__: The wrapper that starts the inference server. In most cases, you can use this file as-is.\n",
    "* __wsgi.py__: The start up shell for the individual server workers. This only needs to be changed if you changed where predictor.py is located or is named.\n",
    "* __predictor.py__: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\n",
    "* __nginx.conf__: The configuration for the nginx master server that manages the multiple workers.\n",
    "    \n",
    "Finally, \n",
    "\n",
    "When SageMaker starts a container, it will invoke the container with an argument of either __train__ or __serve__. We have set this container up so that the argument in treated as the command that the container executes. When training, it will run the __train__ program included and, when serving, it will run the __serve__ program.\n",
    "\n",
    "[dockerfile]: https://docs.docker.com/engine/reference/builder/ \"The official Dockerfile reference guide\"\n",
    "[ecr]: https://aws.amazon.com/ecr/ \"ECR Home Page\"\n",
    "[nginx]: http://nginx.org/\n",
    "[gunicorn]: http://gunicorn.org/\n",
    "[flask]: http://flask.pocoo.org/\n",
    "\n",
    "[An excellent workshop dedicated - to this concept & source of this information](https://sagemaker-workshop.com/custom/code.html).\n",
    "\n",
    "[An even better source](https://github.com/aws/sagemaker-containers/blob/master/README.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerfile dissection \n",
    "\n",
    "The Dockerfile describes the image that you want to build. You can think of it as describing the complete operating system installation of the system that you want to run. A Docker container running is quite a bit lighter than a full operating system, however, because it takes advantage of Linux on the host machine for the basic operations.\n",
    "\n",
    "This Dockerfile is special as we want to access the GPU while training, hence we start off with the Nvidia-Cuda base image\n",
    "\n",
    "```python\n",
    "FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n",
    "ARG ARCH=gpu\n",
    "```\n",
    "\n",
    "After setting up the necessary packages, we then proceed to download the pre-trained models into this container. \n",
    "\n",
    "```python\n",
    "RUN python download_pretrained_models.py --location_dir ./pretrained_models/ --models bert-base-uncased roberta-base distilbert-base-uncased\n",
    "```\n",
    "\n",
    "Amazon SageMaker invokes the training code by running a version of the following command:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run <image> train\n",
    "```\n",
    "\n",
    "This means that your Docker image should have an executable file in it that is called train. You will modify this program to implement your training algorithm. This can be in any language that is capable of running inside of the Docker environment, but the most common language options for data scientists include Python, R, Scala, and Java. For our Scikit example, we use Python.\n",
    "\n",
    "At runtime, Amazon SageMaker injects the training data from an Amazon S3 location into the container. The training program ideally should produce a model artifact. The artifact is written, inside of the container, then packaged into a compressed tar archive and pushed to an Amazon S3 location by Amazon SageMaker.\n",
    "\n",
    "When Amazon SageMaker runs training, your train script is run just like any regular program. A number of files are laid out for your use, under the /opt/ml directory:\n",
    "\n",
    "```bash\n",
    "/opt/ml\n",
    "├── input\n",
    "│   ├── config\n",
    "│   │   ├── hyperparameters.json\n",
    "│   │   └── resourceConfig.json\n",
    "│   └── data\n",
    "│       └── <channel_name>\n",
    "│           └── <input data>\n",
    "├── model\n",
    "│   └── <model files>\n",
    "└── output\n",
    "    └── failure\n",
    "```\n",
    "\n",
    "Lets take a look at the input path, hyperparameters, config for distributed training, data for training - \n",
    "\n",
    "\n",
    "* __/opt/ml/input/config__ contains information to control how your program runs. hyperparameters.json is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. \n",
    "\n",
    "* __resourceConfig.json__ is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn’t support distributed training, we’ll ignore it here.\n",
    "\n",
    "* __/opt/ml/input/data/<channel_name>/__ (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it’s generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure.\n",
    "\n",
    "* __/opt/ml/input/data/<channel_name>_<epoch_number>__ (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "\n",
    "Now this is where the output is directed - \n",
    "\n",
    "\n",
    "* __/opt/ml/model__/ is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the DescribeTrainingJob result.\n",
    "\n",
    "* __/opt/ml/output__ is a directory where the algorithm can write a file failure that describes why the job failed. The contents of this file will be returned in the FailureReason field of the DescribeTrainingJob result. For jobs that succeed, there is no reason to write this file as it will be ignored.\n",
    "\n",
    "More info [here](https://github.com/aws/sagemaker-containers/blob/master/README.rst) & ]here](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-dist-training)\n",
    "\n",
    "\n",
    "```python\n",
    "```\n",
    "```python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick note on Distributed Training Configuration\n",
    "\n",
    "Though we are not using it here, but wanted to mention this - \n",
    "\n",
    "If you're performing distributed training with multiple containers, Amazon SageMaker makes information about all containers available in the /opt/ml/input/config/resourceconfig.json file.\n",
    "\n",
    "To enable inter-container communication, this JSON file contains information for all containers. Amazon SageMaker makes this file available for both FILE and PIPE mode algorithms. The file provides the following information:\n",
    "\n",
    "*     current_host—The name of the current container on the container network. For example, algo-1. Host values can change at any time. Don't write code with specific values for this variable.\n",
    "\n",
    "*    hosts—The list of names of all containers on the container network, sorted lexicographically. For example, [\"algo-1\", \"algo-2\", \"algo-3\"] for a three-node cluster. Containers can use these names to address other containers on the container network. Host values can change at any time. Don't write code with specific values for these variables.\n",
    "\n",
    "*    network_interface_name—The name of the network interface that is exposed to your container. For example, containers running the Message Passing Interface (MPI) can use this information to set the network interface name.\n",
    "\n",
    "*    Do not use the information in /etc/hostname or /etc/hosts because it might be inaccurate.\n",
    "\n",
    "*    Hostname information may not be immediately available to the algorithm container. We recommend adding a retry policy on hostname resolution operations as nodes become available in the cluster.\n",
    "\n",
    "The following is an example file on node 1 in a three-node cluster:\n",
    "\n",
    "```python\n",
    "{\n",
    "\"current_host\": \"algo-1\",\n",
    "\"hosts\": [\"algo-1\",\"algo-2\",\"algo-3\"],\n",
    "\"network_interface_name\":\"eth1\"\n",
    "}\n",
    "```\n",
    "\n",
    "[A helpful blog about distributed training on SageMaker](https://aws.amazon.com/blogs/machine-learning/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker/) and [a workshop](https://sagemaker-workshop.com/builtin/parallelized.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving a model, hosting a model in a container\n",
    "\n",
    "Amazon SageMaker invokes hosting service by running a version of the following command\n",
    "\n",
    "```bash\n",
    "docker run <image> serve\n",
    "```\n",
    "This launches a RESTful API to serve HTTP requests for inference. Again, this can be done in any language or framework that works within the Docker environment.\n",
    "\n",
    "In most Amazon SageMaker containers, serve is simply a wrapper that starts the inference server. Furthermore, Amazon SageMaker injects the model artifact produced in training into the container and unarchives it automatically.\n",
    "\n",
    "Amazon SageMaker uses two URLs in the container:\n",
    "\n",
    "*    __/ping__ will receive GET requests from the infrastructure. Your program returns 200 if the container is up and accepting requests.\n",
    "*    __/invocations__ is the endpoint that receives client inference POST requests. The format of the request and the response is up to the algorithm. If the client supplied ContentType and Accept headers, these will be passed in as well.\n",
    "\n",
    "### About pushing this image to ECR\n",
    "\n",
    "**Amazon SageMaker currently requires Docker images to reside in Amazon ECR**\n",
    "\n",
    "We see these commands in __build_and_push.sh__ \n",
    "\n",
    "For SageMaker to run a container for training or hosting, it needs to be able to find the image hosted in the image repository, Amazon Elastic Container Registry (Amazon ECR). The three main steps to this process are building locally, tagging with the repository location, and pushing the image to the repository.\n",
    "\n",
    "To build the local image, call the following command:\n",
    "\n",
    "```bash\n",
    "docker build <image name>\n",
    "```\n",
    "This takes instructions from the Dockerfile we discussed earlier to generate an image on your local instance. After the image is built, we need to let our local instance of Docker know where to store the image so that SageMaker can find it. We do this by tagging the image with the following command:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker tag <image name> <repository name>\n",
    "```\n",
    "The repository name has the following structure:\n",
    "```bash\n",
    "<account number>.dkr.ecr.<region>.amazonaws.com/<image name>:<tag>\n",
    "```\n",
    "Without tagging the image with the repository name, Docker defaults to uploading to Docker Hub, and not Amazon ECR. Amazon SageMaker currently requires Docker images to reside in Amazon ECR. To push an image to ECR, and not the central Docker registry, you must tag it with the registry hostname.\n",
    "\n",
    "Unlike Docker Hub, Amazon ECR images are private by default, which is a good practice with Amazon SageMaker. If you want to share your Amazon SageMaker images publicly, you can find more information in the Amazon ECR User Guide.\n",
    "\n",
    "Finally, to upload the image to Amazon ECR, with the Region set in the repository name tag, call the following command:\n",
    "```bash\n",
    "docker push <repository name>\n",
    "```\n",
    "One final note on Amazon SageMaker Docker containers. We have already shown you that you have the option to build one Docker container serving both training and hosting, or you can build one for each. While building two Docker images can increase storage requirements and cost due to duplicated common libraries, you might get a benefit from building a significantly smaller inference container, allowing the hosting service to scale more quickly when reacting to traffic increases. This is especially common when you use GPUs for training, but your inference code is optimized for CPUs. You need to consider the tradeoffs when you decide if you want to build a single container or two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have uploaded the container to ECR, lets use it to fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker.predictor import json_serializer\n",
    "import json\n",
    "import numpy as np\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of this data is - [Kaggle Competetion - Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "# Please note - I am not providing the full data-set here, this data set is severly truncated, I would encourage\n",
    "# If you want better results, please do signup for the competetion and use the real dataset. \n",
    "\n",
    "# location for train.csv, val.csv and labels.csv\n",
    "DATA_PATH = Path(\"../sm-data/\")   \n",
    "\n",
    "# Location for storing training_config.json\n",
    "CONFIG_PATH = DATA_PATH/'config'\n",
    "CONFIG_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "suffix = str(np.random.uniform())[4:9]\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = 'toxic-pytorch-sagemaker-' + suffix\n",
    "\n",
    "# Prefix for S3 bucket for input and output\n",
    "prefix = 'toxic_comments/input'\n",
    "prefix_output = 'toxic_comments/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: toxic-pytorch-sagemaker-92090\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://{bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 8e-5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"lr_schedule\": \"warmup_cosine\",\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"optimizer_type\": \"adamw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters \n",
    "\n",
    "* **epochs** - One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "* **lr** - Learning rate - step size to adjust the weights for minimizing the loss function. \n",
    "* **max_seq_length** - Maximum Sequence Length - Maximum number of tokens to input.  \n",
    "* **train_batch_size** - The default is 32, but here its 16. For training we make a parallel reading and shuffle. \n",
    "* **lr_schedule** - The convergence rate and final performance of common deep learning models have significantly benefited from recently proposed heuristics such as learning rate schedules. Using too large learning rate may result in numerical instability especially at the very beginning of the training, where parameters are randomly initialized. The warmup strategy increases the learning rate from 0 to the initial learning rate linearly during the initial N epochs or m batches. After the learning rate warmup stage described earlier, we typically steadily decrease its value from the initial learning rate.  Compared to some widely used strategies including exponential decay and step decay, the cosine decay decreases the learning rate slowly at the beginning, and then becomes almost linear decreasing in the middle, and slows down again at the end. It potentially improves the training progress.\n",
    "* **warmup_steps** - Number of warmup steps. \n",
    "* **optimizer_type** - Here we are choosing Adam, which chooses a diffrent learning rate for each parameter. Helps in speeding up training. AdamW is Adam with L2 regularization and Weight Decay. \n",
    "\n",
    "#### References - source of above statements\n",
    "\n",
    "* [Epochs](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)\n",
    "* [Learning Rate Scheduling](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/)\n",
    "* [A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation](https://openreview.net/forum?id=r14EOsCqKX)\n",
    "* [Bag of Tricks for Image Classification](https://www.dlology.com/blog/bag-of-tricks-for-image-classification-with-convolutional-neural-networks-in-keras/)\n",
    "* [AdamW and Super-convergence is now the fastest way to train neural nets](https://www.fast.ai/2018/07/02/adam-weight-decay/)\n",
    "* [Why AdamW matters](https://towardsdatascience.com/why-adamw-matters-736223f31b5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"run_text\": \"toxic comments\",\n",
    "    \"finetuned_model\": None,\n",
    "    \"do_lower_case\": \"True\",\n",
    "    \"train_file\": \"train.csv\",\n",
    "    \"val_file\": \"val.csv\",\n",
    "    \"label_file\": \"labels.csv\",\n",
    "    \"text_col\": \"comment_text\",\n",
    "    \"label_col\": '[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]',\n",
    "    \"multi_label\": \"True\",\n",
    "    \"grad_accumulation_steps\": \"1\",\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"fp16\": \"True\",\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"logging_steps\": \"300\"\n",
    "}\n",
    "\n",
    "with open(CONFIG_PATH/'training_config.json', 'w') as f:\n",
    "    json.dump(training_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training config \n",
    "\n",
    "Hyperparameters and this training config is consumed in **train** file inside of the container. Like so - \n",
    "\n",
    "```bash\n",
    "hyperparam_path = os.path.join(\n",
    "    prefix, \"input/config/hyperparameters.json\"\n",
    ")  # opt/ml/input/config/hyperparameters.json\n",
    "config_path = os.path.join(\n",
    "    training_config_path, \"training_config.json\"\n",
    ")  # opt/ml/input/data/training/config/training_config.json\n",
    "```\n",
    "\n",
    "* **run_text** - Used for tagging and debugging, logging.\n",
    "* **finetuned_model** - Location of an already fine-tuned model. \n",
    "* **do_lower_case** - Because we are using an uncased model.\n",
    "* **train_file** - name of the train dataset\n",
    "* **val_file** - name of the validation dataset\n",
    "* **label_file** - file where labels are stored.\n",
    "* **text_col** - column where comment text are stored\n",
    "* **label_col** - labels of columns\n",
    "* **multi_label** - We want to do multi-label classification\n",
    "* **grad_accumulation_steps** - Gradient Accumulation If you have small GPUs, you may need to use the gradient accumulation to make training stable. \n",
    "* **fp16_opt_level** - O1 (Conservative Mixed Precision): only some whitelist ops are done in FP16. By switching to 16-bit, we’ll be using half the memory and theoretically less computation at the expense of the available number range and precision. However, pure 16-bit training creates a lot of problems for us (imprecise weight updates, gradient underflow and overflow). Mixed precision training alleviate these problems.\n",
    "* **fp16** - Enabled for the above\n",
    "* **model_type** - RoBERTa. Introduced at Facebook, Robustly optimized BERT approach RoBERTa, is a retraining of BERT with improved training methodology, 1000% more data and compute power.\n",
    "* **model_name** - RoBERTa: A Robustly Optimized BERT Pretraining Approach - 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
    "* **logging_steps** - Control logging granularity\n",
    "\n",
    "\n",
    "#### References - source of above statements\n",
    "\n",
    "* [Multi-Task Deep Neural Networks for Natural Language Understanding](https://awesomeopensource.com/project/namisan/mt-dnn)\n",
    "* [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)\n",
    "* [Use NVIDIA Apex for Easy Mixed Precision Training in PyTorch](https://medium.com/the-artificial-impostor/use-nvidia-apex-for-easy-mixed-precision-training-in-pytorch-46841c6eed8c)\n",
    "* [BERT, RoBERTa, DistilBERT, XLNet — which one to use?](https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8)\n",
    "* [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://github.com/pytorch/fairseq/tree/master/examples/roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://toxic-pytorch-sagemaker-92090/toxic_comments/input/val.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a helper feature to upload data\n",
    "# from your local machine to S3 bucket.\n",
    "\n",
    "s3_input = session.upload_data(DATA_PATH, bucket=bucket , key_prefix=prefix)\n",
    "\n",
    "session.upload_data(str(DATA_PATH/'val.csv'), bucket=bucket , key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://toxic-pytorch-sagemaker-92090/toxic_comments/input/labels.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.upload_data(str(DATA_PATH/'labels.csv'), bucket=bucket , key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://toxic-pytorch-sagemaker-92090/toxic_comments/input/train.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.upload_data(str(DATA_PATH/'train.csv'), bucket=bucket , key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "!aws configure get region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#account = session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "#region = session.boto_session.region_name\n",
    "\n",
    "#image = \"{}.dkr.ecr.{}.amazonaws.com/sagemaker-bert:1.0-gpu-py36\".format(account, region)\n",
    "\n",
    "#Please use only the following images - \n",
    "#US West 2  - 111652037296.dkr.ecr.us-west-2.amazonaws.com/chazarey-sagemaker-fast-bert:1.0-gpu-py36\n",
    "#US East 1 - 111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-sagemaker-fast-bert-copied:1.0-gpu-py36\n",
    "\n",
    "image = \"111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-sagemaker-fast-bert-copied:1.0-gpu-py36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"s3://{}/{}\".format(bucket, prefix_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(image, \n",
    "                                          role,\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.p3.8xlarge', \n",
    "                                          output_path=output_path, \n",
    "                                          base_job_name='toxic-comments',\n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          sagemaker_session=session\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we bring everything together and ask SageMaker to train our container, finetune our BERT\n",
    "\n",
    "* **image** - image_name (str) – The container image to use for training.\n",
    "* **role** - role = sagemaker.get_execution_role() - we got this before, An AWS IAM role (either name or full ARN). The Amazon SageMaker training jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and model artifacts. After the endpoint is created, the inference code might use the IAM role, if it needs to access an AWS resource.\n",
    "* **train_instance_count** - train_instance_count (int) – Number of Amazon EC2 instances to use for training.\n",
    "* **train_instance_type** - train_instance_type (str) – Type of EC2 instance to use for training, for example, ‘ml.c4.xlarge’.\n",
    "* **output_path** - output_path (str) – S3 location for saving the training result (model artifacts and output files). If not specified, results are stored to a default bucket. If the bucket with the specific name does not exist, the estimator creates the bucket during the fit() method execution.\n",
    "* **base_job_name** - base_job_name (str) – Prefix for training job name when the fit() method launches. If not specified, the estimator generates a default job name, based on the training image name and current timestamp.\n",
    "* **hyperparameters** - hyperparameters (dict) – Dictionary containing the hyperparameters to initialize this estimator with. We set this up in the previous cells\n",
    "* **sagemaker_session** - sagemaker_session (sagemaker.session.Session) – Session object which manages interactions with Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one using the default AWS configuration chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-05 00:47:56 Starting - Starting the training job...\n",
      "2019-12-05 00:47:58 Starting - Launching requested ML instances......\n",
      "2019-12-05 00:48:59 Starting - Preparing the instances for training...\n",
      "2019-12-05 00:49:55 Downloading - Downloading input data...\n",
      "2019-12-05 00:50:11 Training - Downloading the training image............\n",
      "2019-12-05 00:52:25 Training - Training image download completed. Training in progress...\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31m/opt/ml/input/data/training/config/training_config.json\u001b[0m\n",
      "\u001b[31m{'run_text': 'toxic comments', 'finetuned_model': None, 'do_lower_case': 'True', 'train_file': 'train.csv', 'val_file': 'val.csv', 'label_file': 'labels.csv', 'text_col': 'comment_text', 'label_col': '[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]', 'multi_label': 'True', 'grad_accumulation_steps': '1', 'fp16_opt_level': 'O1', 'fp16': 'True', 'model_type': 'roberta', 'model_name': 'roberta-base', 'logging_steps': '300'}\u001b[0m\n",
      "\u001b[31m{'train_batch_size': '16', 'warmup_steps': '1000', 'lr': '8e-05', 'max_seq_length': '512', 'optimizer_type': 'adamw', 'lr_schedule': 'warmup_cosine', 'epochs': '10'}\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - root -   model path used /opt/ml/code/pretrained_models/roberta-base\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - root -   finetuned model not available - loading standard pretrained model\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/code/pretrained_models/roberta-base' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli). Assuming '/opt/ml/code/pretrained_models/roberta-base' is a path or url to a directory containing tokenizer files.\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/code/pretrained_models/roberta-base/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/code/pretrained_models/roberta-base/special_tokens_map.json. We won't load it.\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/code/pretrained_models/roberta-base/tokenizer_config.json. We won't load it.\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   loading file /opt/ml/code/pretrained_models/roberta-base/vocab.json\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   loading file /opt/ml/code/pretrained_models/roberta-base/merges.txt\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - root -   Number of GPUs: 4\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:28 - INFO - root -   label columns: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:29 - INFO - root -   Writing example 0 of 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:30 - INFO - root -   Saving features into cached file /opt/ml/input/data/training/cache/cached_roberta_train_multi_label_512\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:31 - INFO - root -   Writing example 0 of 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:32 - INFO - root -   Saving features into cached file /opt/ml/input/data/training/cache/cached_roberta_dev_multi_label_512\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:33 - INFO - root -   databunch labels: 6\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:33 - INFO - root -   multilabel: True, multilabel type: <class 'bool'>\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:33 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/code/pretrained_models/roberta-base/config.json\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:33 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 6,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:33 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/code/pretrained_models/roberta-base/pytorch_model.bin\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:39 - INFO - transformers.modeling_utils -   Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:39 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[31m/opt/ml/model/tensorboard\u001b[0m\n",
      "\u001b[31mSelected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\u001b[0m\n",
      "\u001b[31mDefaults for this optimization level are:\u001b[0m\n",
      "\u001b[31menabled                : True\u001b[0m\n",
      "\u001b[31mopt_level              : O1\u001b[0m\n",
      "\u001b[31mcast_model_type        : None\u001b[0m\n",
      "\u001b[31mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[31mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[31mmaster_weights         : None\u001b[0m\n",
      "\u001b[31mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[31mProcessing user overrides (additional kwargs that are not None)...\u001b[0m\n",
      "\u001b[31mAfter processing overrides, optimization options are:\u001b[0m\n",
      "\u001b[31menabled                : True\u001b[0m\n",
      "\u001b[31mopt_level              : O1\u001b[0m\n",
      "\u001b[31mcast_model_type        : None\u001b[0m\n",
      "\u001b[31mpatch_torch_functions  : True\u001b[0m\n",
      "\u001b[31mkeep_batchnorm_fp32    : None\u001b[0m\n",
      "\u001b[31mmaster_weights         : None\u001b[0m\n",
      "\u001b[31mloss_scale             : dynamic\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -   ***** Running training *****\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -     Num Epochs = 10\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 64\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -     Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[31m12/05/2019 00:52:45 - INFO - root -     Total optimization steps = 160\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:07 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:07 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:07 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   eval_loss after epoch 1: 0.663224495947361: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   eval_accuracy_thresh after epoch 1: 0.5133333206176758: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   eval_roc_auc after epoch 1: 0.5682817789168555: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   eval_fbeta after epoch 1: 0.05643989518284798: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   lr after epoch 1: 1.28e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   train_loss after epoch 1: 0.6695411540567875\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:09 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:16 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:16 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:16 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   eval_loss after epoch 2: 0.6376195400953293: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   eval_accuracy_thresh after epoch 2: 0.6744999885559082: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   eval_roc_auc after epoch 2: 0.5917823328019639: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   eval_fbeta after epoch 2: 0.05643989518284798: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   lr after epoch 2: 2.56e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   train_loss after epoch 2: 0.6532741375267506\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:18 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:25 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:25 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:25 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   eval_loss after epoch 3: 0.5431191548705101: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   eval_accuracy_thresh after epoch 3: 0.9703333377838135: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   eval_roc_auc after epoch 3: 0.7061354837713594: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   eval_fbeta after epoch 3: 0.05643989518284798: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   lr after epoch 3: 3.8400000000000005e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   train_loss after epoch 3: 0.6046110391616821\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:27 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:34 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:34 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:34 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   eval_loss after epoch 4: 0.2609495297074318: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   eval_accuracy_thresh after epoch 4: 0.9703333377838135: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   eval_roc_auc after epoch 4: 0.7136177575179772: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   eval_fbeta after epoch 4: 0.00038461541407741606: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   lr after epoch 4: 5.12e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   train_loss after epoch 4: 0.4184231087565422\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:36 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:43 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:43 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:43 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   eval_loss after epoch 5: 0.16340998373925686: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   eval_accuracy_thresh after epoch 5: 0.9703333377838135: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   eval_roc_auc after epoch 5: 0.794714160545625: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   eval_fbeta after epoch 5: 0.0: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   lr after epoch 5: 6.4000000000000006e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   train_loss after epoch 5: 0.23165962472558022\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:45 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:52 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:52 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:52 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   eval_loss after epoch 6: 0.13690417911857367: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   eval_accuracy_thresh after epoch 6: 0.9703333377838135: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   eval_roc_auc after epoch 6: 0.8370675546840924: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   eval_fbeta after epoch 6: 0.0: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   lr after epoch 6: 7.680000000000001e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   train_loss after epoch 6: 0.17097410093992949\u001b[0m\n",
      "\u001b[31m12/05/2019 00:53:53 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:00 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:00 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:00 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   eval_loss after epoch 7: 0.11663300544023514: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   eval_accuracy_thresh after epoch 7: 0.9703333377838135: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   eval_roc_auc after epoch 7: 0.9353647922062382: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   eval_fbeta after epoch 7: 0.008168211206793785: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   lr after epoch 7: 8.96e-06\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   train_loss after epoch 7: 0.14697999227792025\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:02 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:09 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:09 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:09 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   eval_loss after epoch 8: 0.09683533664792776: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   eval_accuracy_thresh after epoch 8: 0.9746666550636292: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   eval_roc_auc after epoch 8: 0.9586255543675868: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   eval_fbeta after epoch 8: 0.044631700962781906: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   lr after epoch 8: 1.024e-05\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   train_loss after epoch 8: 0.11414836905896664\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:11 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:18 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:18 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:18 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   eval_loss after epoch 9: 0.08875732030719519: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   eval_accuracy_thresh after epoch 9: 0.9726666808128357: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   eval_roc_auc after epoch 9: 0.9635188494629051: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   eval_fbeta after epoch 9: 0.046989116817712784: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   lr after epoch 9: 1.152e-05\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   train_loss after epoch 9: 0.08902076445519924\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:20 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:27 - INFO - root -   Running evaluation\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:27 - INFO - root -     Num examples = 1000\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:27 - INFO - root -     Batch size = 128\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   eval_loss after epoch 10: 0.07882590685039759: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   eval_accuracy_thresh after epoch 10: 0.9745000004768372: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   eval_roc_auc after epoch 10: 0.9654400781228892: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   eval_fbeta after epoch 10: 0.04804413765668869: \u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   lr after epoch 10: 1.2800000000000001e-05\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   train_loss after epoch 10: 0.07215165463276207\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:28 - INFO - root -   \n",
      "\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:29 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/model_out/config.json\u001b[0m\n",
      "\u001b[31m12/05/2019 00:54:29 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/model_out/pytorch_model.bin\u001b[0m\n",
      "\n",
      "2019-12-05 00:54:31 Uploading - Uploading generated training model\n",
      "2019-12-05 00:56:17 Completed - Training job completed\n",
      "Training seconds: 382\n",
      "Billable seconds: 382\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally start training our model \n",
    "\n",
    "Train a model using the input training dataset.\n",
    "The API calls the Amazon SageMaker CreateTrainingJob API to start model training. The API uses configuration you provided to create the estimator and the specified input training data to send the CreatingTrainingJob request to Amazon SageMaker.\n",
    "This is a synchronous operation. After the model training successfully completes, you can call the deploy() method to host the model using the Amazon SageMaker hosting services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to hosting service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(1, \n",
    "                             'ml.m5.large', \n",
    "                             endpoint_name='bert-toxic-comments', \n",
    "                             serializer=json_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our result for this payload is: [[\"toxic\", 9.29754605749622e-05], [\"insult\", 4.071386865689419e-05], [\"obscene\", 3.189296694472432e-05], [\"severe_toxic\", 2.644667256390676e-05], [\"identity_hate\", 1.8441571228322573e-05], [\"threat\", 1.307918591919588e-05]]\n"
     ]
    }
   ],
   "source": [
    "### Invoke the Endpoint\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "sample_payload='{\"text\": \"this is really really good thanks for recommending!!\"}'\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName='bert-toxic-comments',\n",
    "    Body=sample_payload,\n",
    "    ContentType='application/json'\n",
    ")\n",
    "print('Our result for this payload is: {}'.format(response['Body'].read().decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
